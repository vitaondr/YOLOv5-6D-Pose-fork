{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code trains the the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from utils.datasets import get_hash\n",
    "\n",
    "path_to_data =\"../data/dataset_transformed/drone/\"  # \"../data/real_dataset/drone/\"   #\"../data/dataset_transformed/drone/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cache file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24398378\n",
      "3109893\n",
      "3220248\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "cache_name = [\"train\", \"test\", \"validation\"]\n",
    "for name in cache_name:\n",
    "    with open(path_to_data + name + \".txt\") as t:\n",
    "        images = t.readlines()\n",
    "    images = [x.strip(\"\\n\") for x in images]\n",
    "\n",
    "    cache_dic = {}\n",
    "    for img in images:\n",
    "        label_list = np.array([], ndmin=2)\n",
    "        data = list()\n",
    "        with open(path_to_data + \"labels/\" + img[:-3] + \"txt\") as f:\n",
    "            for line in f:\n",
    "                label_list = np.hstack((label_list, np.array([float(x) for x in line.split(\" \")], ndmin=2, dtype=np.float32)))\n",
    "\n",
    "        data.append(label_list)\n",
    "        data.append(path_to_data + \"mask/\" + img[:-3] + \"jpg\")\n",
    "        data.append((640, 480))\n",
    "        cache_dic[path_to_data + \"images/\" + img] = data\n",
    "\n",
    "\n",
    "    cache_dic[\"hash\"] = get_hash([path_to_data + \"labels/\" + x[:-3] + \"txt\" for x in images] + [path_to_data + \"mask/\" + x[:-3] + \"png\" for x in images] + [path_to_data + \"images/\" + x for x in images])\n",
    "    print(cache_dic[\"hash\"])\n",
    "    cache_dic[\"results\"] = (len(images), 0, 0, 0, 0, len(images), 0, 0, len(images))\n",
    "    cache_dic[\"version\"] = 0.1\n",
    "\n",
    "    torch.save(cache_dic, path_to_data + name + \".cache\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of cv2 reading mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "path = \"../data/dataset_transformed/drone/mask/000000.jpg\"\n",
    "\n",
    "cv2.imread(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'../data/dataset_transformed/drone/images/000046.jpg': [array([[          0,     0.20464,     0.77645,     0.15669,     0.75354,     0.20128,      0.7464,      0.2066,     0.72006,     0.24623,     0.71342,     0.16856,     0.84804,     0.21325,     0.84213,     0.21697,     0.80303,     0.25667,     0.79735,     0.09998,     0.13461,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0046.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000097.jpg': [array([[          0,     0.20574,     0.15233,     0.21741,     0.17168,     0.16705,     0.14723,     0.21703,     0.21436,      0.1716,      0.1939,     0.23027,     0.10214,     0.18187,    0.075602,     0.22872,     0.15017,     0.18489,     0.12791,    0.063215,     0.13876,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0097.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000098.jpg': [array([[          0,     0.18938,     0.59843,     0.21796,     0.59851,      0.1704,     0.59377,     0.19953,     0.65338,     0.15286,     0.65192,     0.21254,     0.54742,     0.16832,     0.54028,      0.1956,     0.60027,     0.15215,     0.59615,    0.065805,     0.11311,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0098.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000083.jpg': [array([[          0,     0.11581,     0.13201,     0.11242,     0.18309,     0.15608,     0.19308,      0.1177,    0.099234,     0.16312,      0.1156,    0.078793,     0.15076,      0.1233,     0.16255,    0.082188,     0.06816,     0.12856,    0.086116,    0.084326,     0.12492,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0083.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000198.jpg': [array([[          0,      0.6862,     0.32632,     0.68497,     0.36823,     0.68653,      0.3725,     0.72174,     0.31992,      0.7212,     0.32712,     0.65047,     0.32615,     0.65391,     0.33266,     0.68539,     0.27754,     0.68694,     0.28702,    0.071267,    0.094958,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0198.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000196.jpg': [array([[          0,     0.36442,     0.84074,     0.37933,     0.87987,     0.38897,     0.84269,     0.38404,      0.8447,     0.39383,     0.80786,     0.33619,     0.86498,     0.34804,     0.82857,     0.33904,     0.82917,     0.35122,     0.79317,    0.057642,    0.086706,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0196.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000111.jpg': [array([[          0,     0.47299,     0.32364,     0.48688,     0.34852,     0.51357,     0.27156,     0.41276,     0.31105,     0.43819,     0.23816,     0.51014,       0.386,     0.53278,     0.32057,     0.44567,     0.35225,     0.46739,     0.28979,     0.12002,     0.14784,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0111.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000100.jpg': [array([[          0,     0.61772,     0.64958,     0.64843,     0.65207,     0.62336,     0.63409,     0.62031,     0.69603,     0.59538,     0.67931,     0.63269,     0.61738,      0.6094,     0.60028,     0.60714,     0.65871,     0.58398,     0.64271,     0.06445,    0.095754,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0100.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000061.jpg': [array([[          0,     0.13547,     0.51118,     0.12412,     0.56963,     0.18371,     0.53321,    0.078676,     0.47329,      0.1416,     0.44036,     0.14301,     0.56707,     0.19438,     0.53532,       0.104,     0.48352,     0.15788,     0.45442,      0.1157,     0.12927,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0061.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000090.jpg': [array([[          0,     0.40669,     0.50535,     0.41055,     0.54148,     0.43552,     0.53368,     0.38434,     0.53035,     0.41069,     0.52165,     0.40867,     0.48781,     0.43295,     0.47903,     0.38305,     0.47288,     0.40862,     0.46306,    0.052473,    0.078419,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0090.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000115.jpg': [array([[          0,     0.92067,     0.28531,     0.97213,     0.29968,     0.97447,     0.25053,     0.92721,     0.27683,     0.93017,     0.22681,     0.91156,     0.32751,     0.91433,     0.28304,     0.87006,     0.30764,     0.87335,     0.26246,     0.10441,      0.1007,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0115.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000015.jpg': [array([[          0,     0.89608,     0.16233,     0.86623,     0.14072,     0.92208,     0.10098,     0.86448,     0.13513,     0.91526,    0.098615,      0.8897,     0.22023,     0.94558,     0.18631,     0.88605,     0.20819,     0.93686,     0.17656,    0.081097,     0.12161,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0015.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000131.jpg': [array([[          0,     0.27057,     0.23426,     0.24541,     0.20969,     0.27286,     0.19493,     0.25394,     0.21665,      0.2794,     0.20289,     0.26804,     0.26515,     0.29457,     0.25045,     0.27478,     0.26808,     0.29943,     0.25437,    0.054026,    0.073148,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0131.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000130.jpg': [array([[          0,     0.15641,     0.39606,     0.21325,     0.44287,     0.20477,     0.40316,     0.12363,     0.46598,     0.10514,     0.42534,     0.19451,     0.36154,     0.18471,     0.31627,     0.10765,      0.3777,    0.088518,     0.33042,     0.12473,     0.14972,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0130.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000105.jpg': [array([[          0,    0.085209,     0.67889,     0.05453,     0.70481,    0.086152,     0.69308,     0.05961,     0.65022,    0.090027,     0.63925,    0.088152,     0.71507,      0.1173,     0.70412,    0.091708,     0.66395,     0.11983,     0.65367,    0.065297,    0.075822,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0105.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000139.jpg': [array([[          0,     0.87667,     0.91166,     0.91216,     0.88442,     0.87345,     0.85722,     0.87689,     0.89269,     0.83916,     0.86396,     0.90609,     0.95525,     0.86783,      0.9235,     0.87065,      0.9662,      0.8334,     0.93259,    0.078758,     0.10899,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0139.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000076.jpg': [array([[          0,     0.51355,     0.10092,     0.53212,      0.1106,     0.50541,    0.098328,      0.5147,     0.14589,     0.48953,     0.13524,     0.53235,    0.061341,     0.50604,    0.048137,     0.51515,    0.099541,     0.49034,     0.08807,    0.042816,    0.097749,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0076.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000155.jpg': [array([[          0,     0.43671,     0.45187,     0.43539,     0.49432,     0.36153,     0.44647,     0.43719,     0.54947,     0.37641,     0.50938,     0.48966,     0.37443,     0.42171,     0.33268,     0.48233,     0.44523,      0.4256,     0.40946,     0.12813,     0.21679,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0155.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000096.jpg': [array([[          0,     0.22625,     0.83046,     0.24905,     0.88737,     0.28544,     0.83622,     0.21588,     0.80937,     0.25598,     0.76332,     0.20478,     0.88624,     0.24253,     0.83801,     0.17246,     0.81301,     0.21367,     0.76929,     0.11299,     0.12405,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0096.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000072.jpg': [array([[          0,     0.37363,     0.16804,     0.39598,     0.21744,      0.3655,     0.17167,     0.35141,     0.22144,      0.3196,     0.17909,     0.41951,     0.14521,     0.39181,    0.095036,     0.37463,     0.15355,     0.34543,     0.10732,    0.099904,      0.1264,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0072.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000145.jpg': [array([[          0,    0.072817,     0.51273,    0.056364,     0.46798,     0.10503,     0.44038,    0.001288,     0.49113,    0.052991,     0.46253,     0.10675,     0.56117,     0.14837,     0.52699,    0.052881,     0.57934,    0.097526,     0.54486,     0.14708,     0.13897,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0145.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000184.jpg': [array([[          0,     0.25667,     0.30544,     0.28778,      0.2903,     0.26277,      0.2375,     0.22669,     0.33076,     0.20067,     0.27664,     0.29982,     0.31988,     0.27774,     0.27328,     0.24621,     0.35633,     0.22335,     0.30871,     0.09915,     0.11884,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0184.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000114.jpg': [array([[          0,     0.35708,       0.751,     0.34709,     0.84191,     0.41369,      0.8084,     0.33103,     0.82268,     0.41052,     0.78389,     0.32849,     0.71516,     0.39011,     0.68882,      0.3109,     0.67615,     0.38336,     0.64703,      0.1028,     0.19488,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0114.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000001.jpg': [array([[          0,     0.78967,     0.16547,     0.83154,      0.1853,     0.87103,     0.13232,     0.76465,     0.11283,     0.80397,    0.061712,     0.78418,      0.2475,     0.81824,     0.20182,     0.72798,     0.18451,      0.7619,     0.14022,     0.14304,     0.18579,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0001.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000181.jpg': [array([[          0,     0.81302,     0.18309,     0.81574,     0.21999,     0.84771,     0.19084,     0.81039,     0.21692,     0.84442,     0.18555,     0.79161,     0.17295,     0.82236,     0.14217,     0.78467,     0.16671,      0.8173,     0.13349,    0.063035,    0.086498,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0181.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000160.jpg': [array([[          0,     0.39601,     0.86374,     0.37658,     0.86073,     0.38797,     0.81174,     0.36059,      0.8821,     0.37215,     0.83481,     0.42583,     0.88411,     0.43436,      0.8349,     0.40727,     0.90366,     0.41626,     0.85622,    0.073768,    0.091916,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0160.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000051.jpg': [array([[          0,     0.88561,     0.57123,     0.90206,     0.55403,     0.87139,     0.53993,     0.91142,     0.55239,     0.87838,     0.53736,     0.88487,     0.60113,     0.85484,     0.58598,     0.89295,     0.60282,     0.86064,     0.58658,    0.056581,    0.065462,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0051.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000135.jpg': [array([[          0,      0.3722,      0.7195,     0.43994,     0.65802,     0.40434,     0.63272,     0.34046,     0.66474,      0.3113,     0.63522,     0.42371,     0.79679,     0.38981,     0.75772,     0.32204,      0.8248,     0.29504,     0.77723,      0.1449,     0.19207,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0135.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000195.jpg': [array([[          0,     0.41133,    0.093473,     0.45707,    0.071796,     0.41754,   0.0096654,     0.41047,     0.15147,     0.36898,    0.095186,     0.43893,    0.080197,     0.40308,    0.025794,     0.39773,       0.151,     0.36036,     0.10118,    0.096714,     0.14181,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0195.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000005.jpg': [array([[          0,      0.4992,     0.62841,     0.52208,     0.63644,      0.5218,     0.59938,     0.48094,     0.63403,     0.48173,     0.59671,      0.5157,     0.65053,     0.51559,     0.61564,      0.4774,      0.6484,     0.47821,     0.61329,    0.044687,    0.053818,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0005.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000177.jpg': [array([[          0,       0.773,     0.89586,     0.84105,     0.97302,     0.81301,     0.85679,     0.72129,     0.96451,      0.6999,      0.8602,     0.83835,     0.90587,     0.81488,     0.81026,     0.73687,     0.90455,     0.71818,     0.81716,     0.14116,     0.16276,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0177.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000053.jpg': [array([[          0,     0.83953,    0.073489,     0.87118,    0.063215,     0.88361,    0.019021,      0.8578,     0.11731,     0.86955,    0.076006,     0.81386,    0.062064,      0.8221,    0.019716,     0.80013,      0.1139,     0.80761,    0.074197,    0.083488,    0.098286,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0053.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000026.jpg': [array([[          0,     0.48245,     0.19039,     0.50367,     0.13419,     0.51861,     0.15762,     0.44917,     0.14806,     0.46735,     0.16954,     0.50343,     0.22052,     0.51827,     0.23744,     0.44942,     0.23046,     0.46744,     0.24598,    0.069445,     0.11179,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0026.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000055.jpg': [array([[          0,     0.55081,     0.15144,     0.58139,     0.19635,     0.57773,     0.15458,     0.58557,     0.12794,     0.58133,     0.08332,     0.52093,     0.20399,     0.51899,     0.16236,     0.51763,     0.13719,     0.51556,    0.092695,    0.070003,     0.12067,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0055.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000050.jpg': [array([[          0,     0.54142,     0.79288,     0.57361,     0.80993,     0.57504,     0.79403,     0.53436,     0.84397,     0.53383,     0.82933,     0.54745,     0.75626,     0.54758,     0.73879,     0.50974,     0.78692,     0.50806,     0.77042,    0.066982,     0.10518,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0050.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000174.jpg': [array([[          0,     0.82109,     0.83552,     0.83112,     0.87376,      0.8017,     0.84077,     0.81037,     0.86805,     0.78354,     0.83763,     0.85541,     0.82456,     0.82607,     0.79329,     0.83325,     0.82261,     0.80648,     0.79366,    0.071864,    0.080468,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0174.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000148.jpg': [array([[          0,     0.49893,      0.1635,     0.50931,     0.14641,     0.48564,      0.1198,     0.52996,     0.14421,     0.50521,     0.11541,     0.48751,     0.20143,     0.46342,     0.17713,     0.50621,     0.20339,     0.48095,     0.17726,    0.066541,    0.087981,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0148.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000188.jpg': [array([[          0,     0.86277,    0.084291,     0.89804,    0.069223,     0.88922,     0.04773,     0.86116,     0.06537,     0.85317,    0.043886,     0.86951,     0.11905,     0.86187,    0.097857,     0.83496,     0.11563,     0.82805,    0.094439,    0.069985,    0.075163,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0188.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000151.jpg': [array([[          0,     0.59957,     0.66024,     0.63483,     0.69501,     0.60925,     0.70451,     0.59296,      0.6929,     0.56755,     0.70152,     0.62602,     0.61807,     0.60047,     0.62355,     0.58552,       0.622,      0.5602,     0.62721,    0.074636,    0.086441,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0151.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000141.jpg': [array([[          0,     0.76484,     0.62898,     0.78883,     0.65624,     0.79594,     0.61957,     0.74509,     0.64166,     0.75181,      0.6057,      0.7784,     0.64268,     0.78493,     0.60867,     0.73804,     0.62943,     0.74424,     0.59603,    0.057898,    0.060205,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0141.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000086.jpg': [array([[          0,      0.4918,     0.46445,     0.45318,     0.48835,     0.42363,     0.45196,     0.49404,     0.54437,     0.46486,     0.50804,     0.51751,     0.40139,     0.48335,     0.37225,     0.55036,      0.4668,     0.51762,     0.43628,     0.12673,     0.17212,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0086.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000120.jpg': [array([[          0,     0.87019,     0.53114,     0.88077,     0.57921,     0.91641,      0.5666,     0.83563,     0.55645,     0.86843,     0.54207,     0.87904,     0.51667,     0.91301,     0.50037,     0.83576,     0.49424,     0.86711,     0.47623,    0.080779,     0.10298,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0120.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000191.jpg': [array([[          0,     0.45895,     0.47634,     0.48921,     0.38623,     0.48188,     0.39848,     0.39137,     0.43642,     0.39647,     0.44248,     0.52782,     0.51696,      0.5158,     0.51282,     0.43248,     0.56957,     0.43229,     0.55867,     0.13645,     0.18334,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0191.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000183.jpg': [array([[          0,     0.76925,     0.49013,     0.80236,     0.49573,     0.79617,      0.4603,     0.76618,     0.49818,     0.76039,     0.46231,     0.77563,     0.50851,     0.77021,     0.47546,     0.74181,     0.51091,     0.73675,     0.47748,    0.065611,    0.050612,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0183.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000037.jpg': [array([[          0,    0.079024,     0.14113,    0.077768,    0.092909,    0.043325,    0.077629,    0.059344,     0.16053,    0.024907,     0.14502,     0.12157,     0.13525,    0.090426,     0.12137,     0.10504,     0.19618,    0.073905,     0.18211,    0.096661,     0.11855,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0037.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000002.jpg': [array([[          0,     0.64523,     0.18006,     0.66522,        0.15,        0.68,     0.14721,      0.6227,     0.14449,     0.63843,     0.14206,     0.65572,     0.22025,     0.67014,     0.21486,     0.61572,     0.21166,     0.63098,     0.20683,    0.064282,    0.078194,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0002.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000127.jpg': [array([[          0,     0.22665,    0.081273,     0.19572,     -0.0268,     0.22492,   -0.013289,     0.15144,     0.08143,     0.18336,     0.08356,     0.28254,    0.081695,     0.30143,    0.083782,     0.23618,     0.17653,     0.25837,     0.16975,        0.15,     0.20333,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0127.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000157.jpg': [array([[          0,     0.50481,    0.067695,      0.5027,    0.028716,     0.52232,    0.021243,     0.47771,    0.052697,     0.49683,    0.045034,     0.51818,    0.091105,     0.53674,    0.082175,     0.49306,     0.11063,     0.51119,     0.10171,    0.059033,    0.089383,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0157.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000011.jpg': [array([[          0,     0.54836,     0.76119,     0.58244,     0.74311,     0.56865,     0.71709,     0.53505,     0.74012,     0.52394,     0.71354,     0.57009,     0.80447,     0.55731,      0.7751,     0.52324,      0.8034,      0.5131,     0.77326,    0.069338,    0.090922,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0011.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000064.jpg': [array([[          0,     0.91647,     0.80152,      1.0096,     0.82601,      0.9634,     0.74863,     0.94394,     0.77976,      0.9029,     0.70432,     0.91903,      0.8797,     0.88508,     0.80385,     0.85626,      0.8399,     0.82714,     0.76526,     0.18243,     0.17538,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0064.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000154.jpg': [array([[          0,     0.23044,      0.2638,     0.24336,     0.33192,     0.28618,     0.30554,     0.20934,     0.29339,     0.25889,     0.26357,     0.21508,     0.25624,     0.25768,     0.23094,     0.17737,     0.20709,     0.22661,     0.17869,     0.10881,     0.15323,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0154.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000067.jpg': [array([[          0,     0.76649,     0.82372,     0.81304,     0.86328,     0.78579,     0.87003,     0.76222,     0.86494,     0.73563,     0.87115,     0.79072,     0.77532,     0.76416,     0.77855,     0.74418,     0.78407,     0.71833,     0.78731,    0.094719,    0.095824,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0067.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000063.jpg': [array([[          0,     0.69508,     0.45631,     0.72691,     0.53266,     0.78192,     0.48455,      0.6605,     0.52349,     0.71395,     0.46917,     0.69022,      0.4337,     0.73655,     0.38113,     0.62697,     0.41675,     0.67139,     0.35741,     0.15495,     0.17525,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0063.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000152.jpg': [array([[          0,     0.36762,     0.13798,     0.37938,     0.15004,     0.35611,     0.10393,     0.34935,      0.1807,     0.32644,     0.13828,     0.40281,     0.12729,     0.38178,    0.083356,     0.37329,     0.15738,     0.35249,     0.11678,    0.076371,    0.097348,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0152.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000027.jpg': [array([[          0,     0.84135,     0.81533,     0.87751,     0.81628,      0.8578,     0.77992,     0.84935,      0.8453,     0.82973,     0.80763,     0.84723,     0.81435,     0.82939,     0.78054,     0.82049,     0.84123,     0.80273,      0.8063,    0.074774,    0.065377,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0027.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000004.jpg': [array([[          0,     0.45838,     0.83174,     0.47137,     0.85617,      0.4983,     0.86269,     0.43026,     0.88263,     0.45731,      0.8912,     0.46489,     0.78018,     0.49095,     0.78279,     0.42411,     0.79979,     0.45024,     0.80369,    0.074187,     0.11102,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0004.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000133.jpg': [array([[          0,      0.6663,     0.32938,     0.69413,     0.31452,     0.67492,     0.28749,     0.66433,     0.35051,     0.64524,     0.32344,     0.68145,     0.32934,     0.66372,      0.3043,     0.65392,      0.3626,     0.63628,     0.33753,    0.057853,    0.075107,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0133.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000109.jpg': [array([[          0,     0.47121,     0.48358,     0.50276,     0.49606,      0.4609,     0.48491,     0.47657,     0.54192,     0.43731,     0.53337,     0.49513,     0.43109,     0.45613,      0.4184,     0.47128,     0.47884,     0.43455,     0.46876,    0.068207,     0.12351,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0109.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000000.jpg': [array([[          0,       0.847,      0.3139,     0.89152,     0.36661,      0.9347,     0.27399,     0.88452,     0.38203,     0.93491,     0.27398,     0.78923,     0.32255,     0.82186,     0.23589,     0.76764,     0.32981,     0.80393,     0.22976,     0.16727,     0.15227,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0000.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000049.jpg': [array([[          0,     0.47278,     0.72321,     0.50374,     0.76309,     0.50439,      0.7815,     0.44064,     0.75985,     0.43518,     0.77796,     0.50615,     0.67499,     0.50702,     0.68473,     0.44392,     0.67178,     0.43886,     0.68122,    0.071839,     0.10972,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0049.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000162.jpg': [array([[          0,     0.38138,     0.16461,     0.35364,     0.16486,     0.39787,      0.1255,      0.3378,     0.11574,      0.3787,    0.078589,     0.39388,      0.2391,     0.43336,     0.20522,     0.37726,      0.1907,     0.41407,     0.15844,    0.095562,     0.16051,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0162.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000009.jpg': [array([[          0,     0.63739,      0.4838,     0.66309,     0.53518,     0.64871,     0.52389,     0.60849,     0.51777,     0.59151,     0.50615,     0.68012,     0.45636,     0.66707,     0.43951,     0.62472,     0.44449,     0.60895,     0.42809,     0.08861,     0.10709,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0009.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000159.jpg': [array([[          0,     0.75437,     0.46587,     0.78901,     0.37815,     0.71525,     0.37347,     0.80712,     0.45181,     0.72065,     0.44337,     0.76497,     0.48432,     0.69986,     0.47682,     0.77722,     0.56432,      0.7024,     0.55288,     0.10726,     0.19085,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0159.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000034.jpg': [array([[          0,     0.86639,     0.70814,     0.89358,     0.77555,     0.87183,      0.7528,     0.82869,     0.73328,     0.80717,     0.71164,     0.92568,     0.69688,     0.90538,     0.67136,     0.85791,     0.66345,     0.83757,     0.63964,     0.11851,      0.1359,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0034.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000158.jpg': [array([[          0,     0.82406,     0.30886,     0.83903,     0.40827,     0.82324,     0.41329,     0.93168,     0.27511,      0.9023,     0.29854,     0.73879,     0.32192,     0.73596,     0.33715,     0.81876,     0.19114,     0.80549,     0.22408,     0.19571,     0.22214,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0158.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000016.jpg': [array([[          0,     0.31644,     0.70315,     0.32214,     0.73948,      0.3183,     0.71584,     0.28995,     0.72318,     0.28522,     0.69952,     0.34521,     0.70036,     0.34227,     0.67703,     0.31448,     0.68572,     0.31072,      0.6624,    0.059989,    0.077073,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0016.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000143.jpg': [array([[          0,     0.60764,     0.14339,     0.62571,     0.22441,     0.65935,     0.19208,     0.55483,     0.19369,     0.58163,     0.15575,     0.63633,      0.1219,      0.6709,    0.077992,     0.56656,    0.086338,     0.59454,    0.035638,     0.11607,     0.18878,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0143.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000144.jpg': [array([[          0,     0.12532,      0.7684,     0.13527,     0.82887,    0.032154,     0.81375,     0.13181,      0.8679,     0.04287,      0.8572,     0.18393,     0.67854,    0.099096,     0.65687,     0.17529,        0.73,     0.10029,      0.7136,     0.15177,     0.21103,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0144.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000077.jpg': [array([[          0,     0.34153,     0.72623,     0.35702,     0.78835,     0.28518,     0.78975,     0.36509,     0.78929,     0.30431,     0.79049,     0.36625,     0.65451,     0.30001,     0.65316,     0.37251,     0.67409,     0.31579,     0.67326,     0.08733,     0.13733,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0077.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000101.jpg': [array([[          0,     0.63801,     0.24895,     0.63171,     0.25106,      0.6186,     0.21424,     0.61531,     0.28654,     0.60348,     0.25232,     0.67331,     0.23638,     0.65992,     0.20045,     0.65381,     0.27243,     0.64173,     0.23898,    0.069828,    0.086093,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0101.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000018.jpg': [array([[          0,     0.72196,     0.48093,     0.70207,     0.52498,     0.74214,     0.48235,     0.67188,     0.45467,     0.70672,     0.41134,     0.74593,     0.53827,     0.78572,     0.49963,     0.71443,     0.47153,      0.7493,     0.43213,     0.11385,     0.12693,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0018.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000020.jpg': [array([[          0,     0.32276,     0.64559,     0.31819,     0.60008,     0.32314,     0.55639,     0.25656,     0.67004,     0.26721,     0.62067,     0.38302,     0.66138,     0.38182,     0.61546,     0.32492,     0.72447,     0.32882,     0.67393,     0.12646,     0.16808,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0020.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000118.jpg': [array([[          0,     0.33739,     0.87284,     0.35124,     0.89296,     0.35998,     0.84131,     0.30282,     0.88598,     0.31334,     0.83486,     0.36296,     0.89819,     0.37063,     0.85021,      0.3182,     0.89172,     0.32741,      0.8442,    0.067815,     0.06333,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0118.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000017.jpg': [array([[          0,     0.44538,    0.094707,     0.48876,     0.12616,     0.49262,     0.07099,     0.43753,     0.15827,     0.43686,     0.10378,     0.45186,    0.073559,     0.45245,    0.015548,     0.40076,     0.10284,     0.39684,    0.045127,    0.095777,     0.14272,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0017.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000084.jpg': [array([[          0,     0.62063,       0.414,     0.62438,      0.4515,     0.64819,     0.43373,     0.61531,     0.42346,     0.64052,     0.40433,     0.60778,     0.41786,     0.63038,     0.40062,     0.59817,     0.38918,     0.62202,     0.37066,    0.050019,    0.080838,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0084.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000073.jpg': [array([[          0,     0.60623,     0.20369,     0.60989,     0.24375,     0.63373,     0.22411,     0.59138,     0.21549,     0.61606,     0.19381,     0.60256,     0.20705,     0.62545,     0.18698,     0.58424,     0.17765,     0.60789,     0.15556,    0.049487,    0.088193,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0073.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000193.jpg': [array([[          0,     0.46195,     0.39742,     0.48956,     0.40567,     0.49012,     0.36685,     0.47613,     0.42754,     0.47629,     0.38634,      0.4483,     0.39829,     0.44759,     0.36001,     0.43202,     0.41915,     0.43073,     0.37855,     0.05939,     0.06753,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0193.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000071.jpg': [array([[          0,     0.92531,     0.61307,     0.93622,     0.55917,     0.88183,     0.53699,      1.0016,     0.58762,     0.93705,     0.56127,     0.90225,     0.65329,     0.84963,     0.63093,     0.95971,     0.69692,     0.89762,     0.67035,     0.15198,     0.15993,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0071.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000070.jpg': [array([[          0,     0.72569,     0.27956,     0.78747,     0.30018,     0.82342,      0.2363,     0.69243,     0.21379,     0.73151,     0.15582,     0.72718,     0.37845,     0.75914,     0.32277,     0.65063,     0.30523,      0.6846,     0.25374,     0.17279,     0.22263,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0070.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000175.jpg': [array([[          0,     0.33672,     0.14508,     0.37686,      0.1287,     0.35168,     0.08103,     0.33263,     0.14776,       0.305,    0.096978,     0.35765,     0.17669,     0.33437,      0.1337,     0.31615,     0.19712,      0.2908,     0.15161,    0.086056,     0.11609,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0175.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000122.jpg': [array([[          0,     0.49516,     0.56413,     0.51353,     0.59432,     0.50751,     0.57376,     0.47578,     0.59354,      0.4683,     0.57341,     0.51894,     0.54978,     0.51344,     0.52841,     0.48284,     0.54994,     0.47601,     0.52904,    0.050635,    0.065909,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0122.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000060.jpg': [array([[          0,     0.42022,     0.08451,     0.39783,   -0.057334,     0.36076,  0.00060742,     0.37307,    0.094714,      0.3383,     0.13576,     0.49182,    0.046877,     0.44986,     0.08707,     0.47203,     0.17918,     0.43151,     0.20638,     0.15352,     0.26372,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0060.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000182.jpg': [array([[          0,     0.68068,     0.12669,     0.70211,     0.14018,     0.67801,     0.13476,     0.67819,     0.16643,     0.65549,     0.16176,     0.70174,    0.086975,     0.67768,    0.080775,     0.67788,     0.11694,     0.65521,      0.1116,    0.046901,    0.085657,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0182.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000021.jpg': [array([[          0,     0.14964,     0.45923,      0.1621,     0.37523,     0.22849,     0.39075,    0.077058,     0.38729,     0.15089,      0.4004,     0.16668,     0.53605,     0.23234,     0.52952,    0.081504,      0.5381,     0.15465,     0.53167,     0.15528,     0.16288,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0021.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000042.jpg': [array([[          0,     0.44341,     0.14046,     0.44952,     0.11352,     0.42402,     0.10407,      0.4424,     0.17082,     0.41788,     0.16132,     0.46305,     0.11804,     0.43913,     0.10918,     0.45593,     0.17154,     0.43286,     0.16263,    0.045176,    0.067469,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0042.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000031.jpg': [array([[          0,     0.30862,     0.52233,     0.28842,     0.59701,     0.29428,     0.59182,     0.24853,     0.50457,     0.25016,     0.48938,     0.36025,     0.54596,     0.37265,      0.5362,     0.32399,     0.45998,     0.33293,     0.44164,     0.12412,     0.15537,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0031.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000171.jpg': [array([[          0,     0.24523,     0.62677,     0.29978,     0.61581,     0.26001,     0.57714,     0.24548,     0.70013,     0.19951,     0.66534,     0.27266,     0.58773,      0.2353,     0.55131,     0.22257,     0.66215,     0.18008,     0.62867,      0.1197,     0.14882,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0171.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000054.jpg': [array([[          0,     0.86926,     0.69918,     0.91529,     0.75528,     0.85089,     0.74479,     0.87912,     0.75232,     0.82364,     0.74321,     0.90575,     0.64521,     0.84452,     0.63826,     0.87221,     0.65583,      0.8191,     0.64945,    0.096185,     0.11703,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0054.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000110.jpg': [array([[          0,     0.28897,     0.63571,     0.33076,     0.71516,     0.24415,     0.72172,     0.29666,     0.69961,     0.22106,     0.70374,     0.34104,      0.5614,     0.25941,     0.55217,     0.30667,     0.56847,     0.23482,     0.56128,     0.11998,     0.16955,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0110.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000173.jpg': [array([[          0,     0.66571,    0.097809,     0.68344,     0.13615,     0.70621,     0.10142,     0.64979,     0.13756,     0.67142,     0.10145,     0.66491,    0.085309,     0.68624,     0.04811,     0.63106,    0.084712,     0.65118,    0.045933,    0.075148,    0.091632,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0173.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000186.jpg': [array([[          0,     0.18036,     0.26535,     0.18416,     0.31037,     0.20372,      0.2603,     0.12402,     0.25771,     0.14274,     0.20505,     0.21707,     0.30612,     0.23513,     0.26148,     0.16437,     0.25919,     0.18179,      0.2125,     0.11111,     0.10531,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0186.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000078.jpg': [array([[          0,     0.11094,     0.56108,     0.14299,     0.53464,     0.12546,     0.52163,    0.093298,     0.54417,     0.07555,     0.52988,     0.13977,     0.58808,     0.12264,     0.57414,    0.090864,     0.60174,    0.073542,     0.58637,    0.069452,    0.080108,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0078.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000085.jpg': [array([[          0,     0.18525,      0.6536,     0.19883,     0.64901,     0.22881,     0.63242,     0.15957,     0.61453,     0.19153,      0.6005,     0.18581,     0.70726,     0.21588,     0.68733,     0.14786,     0.67254,     0.17976,     0.65518,    0.080954,     0.10676,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0085.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000121.jpg': [array([[          0,     0.15087,     0.76705,     0.16782,     0.79134,     0.16958,     0.79752,     0.12247,     0.79284,     0.12227,     0.79937,     0.17705,     0.73785,     0.17916,     0.74166,     0.13302,     0.73708,     0.13329,     0.74102,    0.056891,    0.062281,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0121.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000099.jpg': [array([[          0,     0.45267,     0.80897,      0.4723,     0.80075,     0.46326,     0.75966,     0.42666,     0.82707,     0.41966,     0.78439,     0.48316,     0.82387,     0.47421,     0.78433,      0.4406,     0.84868,     0.43343,     0.80772,    0.063501,    0.089021,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0099.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000007.jpg': [array([[          0,     0.19239,     0.68637,     0.20823,     0.72133,     0.21491,     0.71569,     0.15887,     0.71558,     0.16365,     0.70939,     0.21915,     0.66331,     0.22602,     0.65549,      0.1723,     0.65505,     0.17747,     0.64648,    0.067146,     0.07485,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0007.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000136.jpg': [array([[          0,     0.25814,     0.26762,     0.28478,      0.3323,     0.26138,     0.30809,     0.22459,     0.30425,     0.19529,     0.27741,     0.31011,     0.25038,     0.28956,     0.21823,     0.25042,     0.22331,     0.22408,     0.18872,     0.11482,     0.14358,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0136.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000068.jpg': [array([[          0,     0.68612,     0.27463,     0.67466,     0.31172,     0.68546,     0.28689,     0.65576,     0.27163,     0.66526,     0.24533,     0.70947,     0.29656,     0.72199,     0.27128,     0.68995,     0.25739,     0.70112,     0.23073,    0.066238,    0.080991,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0068.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000113.jpg': [array([[          0,    0.073862,     0.42278,    0.077686,     0.45945,    0.043683,     0.44326,    0.072575,     0.44274,     0.04006,     0.42665,     0.10069,     0.41383,    0.067867,      0.3957,    0.094497,     0.39938,    0.063054,     0.38154,    0.060627,    0.077907,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0113.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000192.jpg': [array([[          0,     0.32736,     0.19975,     0.30647,     0.10285,     0.29357,    0.072829,     0.25559,     0.24015,     0.24815,     0.20019,     0.40714,     0.19239,     0.38763,     0.15898,     0.35613,     0.30481,     0.34183,     0.26484,     0.15899,     0.23198,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0192.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000168.jpg': [array([[          0,     0.91779,     0.74727,     0.95318,     0.82138,     0.98012,     0.78296,     0.86848,     0.77393,      0.8887,     0.73529,     0.94941,     0.74806,     0.97357,     0.70916,     0.87213,     0.70884,     0.89074,      0.6702,     0.11164,     0.15118,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0168.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000043.jpg': [array([[          0,     0.79462,     0.69653,     0.82552,     0.67302,     0.80439,     0.66664,     0.82285,     0.73487,     0.80127,     0.72989,     0.78317,     0.66555,     0.76277,     0.65951,     0.78003,     0.72243,     0.75922,     0.71757,    0.066306,    0.075357,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0043.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000179.jpg': [array([[          0,     0.39855,     0.66379,     0.38402,     0.71697,     0.42248,     0.71278,     0.37891,     0.65592,     0.41899,     0.65208,     0.38889,     0.67234,     0.42376,     0.66886,     0.38445,     0.61551,     0.42065,     0.61235,    0.044846,     0.10462,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0179.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000047.jpg': [array([[          0,     0.42302,     0.53388,     0.44042,     0.49966,     0.42533,     0.49682,      0.4051,     0.51994,     0.39089,     0.51606,     0.45115,     0.55142,     0.43595,     0.54647,     0.41722,     0.57374,      0.4028,     0.56757,    0.060263,     0.07692,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0047.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000057.jpg': [array([[          0,     0.64358,     0.33869,     0.64586,     0.32081,     0.67152,     0.31608,     0.62616,     0.30857,      0.6504,     0.30427,     0.64227,     0.37453,     0.66696,     0.36939,     0.62353,     0.35941,      0.6469,     0.35473,    0.047987,     0.07026,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0057.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000019.jpg': [array([[          0,     0.34393,      0.5473,     0.31535,     0.56104,     0.30956,     0.52724,     0.33699,     0.58279,     0.33099,     0.55073,     0.35765,     0.53482,     0.35056,      0.5019,     0.37619,     0.55842,     0.36906,     0.52713,     0.06663,    0.080883,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0019.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000134.jpg': [array([[          0,     0.17513,     0.79747,      0.2062,     0.83258,     0.17022,     0.85561,     0.16849,      0.8187,     0.13115,     0.83984,     0.20928,     0.75933,     0.17364,     0.77678,     0.17156,     0.74807,     0.13455,     0.76405,    0.078131,     0.10754,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0134.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000180.jpg': [array([[          0,     0.23993,     0.50787,     0.21748,     0.58452,     0.27551,     0.56913,     0.23898,      0.5395,      0.3016,     0.52623,     0.19527,     0.48606,     0.25292,     0.47846,     0.21331,     0.43153,     0.27561,     0.42758,     0.10633,     0.15694,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0180.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000024.jpg': [array([[          0,     0.63465,      0.1171,     0.62137,     0.15835,      0.6245,     0.12157,     0.60519,     0.12191,     0.60729,    0.084406,     0.66331,     0.14002,     0.66889,     0.10219,      0.6457,     0.10423,     0.65009,    0.065743,    0.063705,    0.092606,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0024.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000003.jpg': [array([[          0,     0.29986,     0.57378,     0.26622,     0.62191,     0.26543,     0.61265,     0.26577,     0.54088,     0.26497,     0.52547,     0.33355,     0.61489,     0.33807,     0.60557,       0.331,     0.53802,     0.33517,      0.5232,    0.073104,    0.098715,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0003.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000156.jpg': [array([[          0,     0.91242,     0.38961,     0.99052,     0.39935,     0.95792,     0.35901,     0.90278,     0.46658,     0.87034,     0.43259,     0.94442,     0.33916,     0.91393,     0.30133,     0.86943,     0.40484,      0.8393,     0.37242,     0.15123,     0.16526,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0156.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000161.jpg': [array([[          0,     0.51474,     0.21156,     0.56354,     0.24541,     0.52552,     0.19383,     0.49704,     0.29542,     0.45738,     0.25343,     0.56092,     0.15869,     0.52538,     0.10322,     0.49862,     0.21356,     0.46164,     0.16782,     0.10617,     0.19221,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0161.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000008.jpg': [array([[          0,    0.080442,     0.63271,    0.057867,     0.60713,     0.03848,     0.57133,    0.030614,      0.6927,    0.012723,     0.65368,     0.14121,     0.60504,     0.12079,     0.57349,     0.11604,     0.68041,    0.096877,     0.64636,     0.12849,     0.12138,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0008.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000029.jpg': [array([[          0,    0.068241,     0.13023,    0.025216,     0.13095,     0.02463,    0.094094,    0.062681,     0.11287,    0.063677,    0.077547,    0.074176,     0.17546,    0.075694,     0.14161,     0.10778,      0.1553,     0.11056,     0.12269,    0.085928,    0.097908,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0029.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000197.jpg': [array([[          0,     0.79863,     0.64831,     0.81554,     0.62101,     0.78725,     0.61647,     0.80315,      0.6308,     0.77394,     0.62566,     0.81628,     0.67046,     0.78806,     0.66411,     0.80398,     0.68276,     0.77484,     0.67562,    0.042335,    0.066286,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0197.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000194.jpg': [array([[          0,     0.40198,     0.79452,     0.33856,       0.813,     0.34942,     0.84526,     0.38618,      0.7116,     0.40173,     0.73091,     0.40737,     0.85837,     0.42512,     0.89363,     0.44934,      0.7603,     0.47058,     0.78356,     0.13202,     0.18203,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0194.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000030.jpg': [array([[          0,     0.41045,     0.12418,     0.43018,     0.17001,     0.43525,     0.13305,     0.38206,      0.1535,     0.38412,     0.11456,     0.43473,     0.12316,      0.4399,    0.084654,     0.38839,     0.10619,     0.39076,    0.065718,    0.057832,      0.1043,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0030.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000048.jpg': [array([[          0,     0.22841,     0.26871,     0.24292,     0.32031,      0.2598,      0.2777,     0.18248,     0.27566,     0.19815,       0.229,     0.25755,     0.29223,     0.27343,     0.25242,     0.20315,     0.25007,     0.21808,      0.2068,    0.090948,     0.11351,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0048.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000167.jpg': [array([[          0,     0.29692,     0.54962,     0.26107,     0.60319,     0.30371,     0.59144,     0.30855,     0.60136,      0.3502,     0.58891,     0.25285,     0.50559,     0.29614,     0.50164,     0.29968,     0.49604,     0.34208,     0.49262,    0.097355,     0.11057,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0167.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000095.jpg': [array([[          0,     0.58888,     0.73391,     0.60913,     0.73552,     0.57093,     0.72152,     0.59576,     0.79021,     0.55923,     0.77629,     0.60905,     0.68944,     0.57441,     0.67713,      0.5969,     0.74089,     0.56363,     0.72861,    0.049898,     0.11308,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0095.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000022.jpg': [array([[          0,     0.39361,     0.79645,     0.39309,     0.80558,     0.41933,     0.76471,     0.35769,     0.77588,     0.38399,     0.73872,     0.41025,      0.8489,     0.43434,     0.80824,     0.37571,     0.81823,     0.39992,     0.78113,    0.076644,     0.11018,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0022.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000032.jpg': [array([[          0,     0.41276,     0.21498,     0.42344,     0.26365,     0.44983,      0.2391,     0.39715,     0.23623,     0.42548,     0.20872,     0.40712,     0.21357,      0.4326,     0.18857,     0.37997,     0.18223,     0.40723,     0.15421,    0.069864,     0.10943,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0032.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000079.jpg': [array([[          0,      0.5812,     0.71223,     0.60655,     0.67519,     0.58828,     0.66346,     0.56088,     0.69416,     0.54501,     0.68079,     0.61438,     0.74257,     0.59575,     0.72666,     0.56934,     0.76375,     0.55304,     0.74593,    0.069369,      0.1003,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0079.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000187.jpg': [array([[          0,     0.16067,     0.30436,     0.19367,     0.35482,     0.19791,     0.34464,     0.12192,      0.3422,     0.12114,     0.33029,     0.19537,     0.27709,     0.19973,     0.26156,     0.12389,     0.25886,     0.12326,     0.24076,     0.07859,     0.11406,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0187.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000150.jpg': [array([[          0,     0.85515,     0.30391,      0.8482,     0.34601,     0.87846,     0.31783,     0.87106,     0.33376,      0.9047,     0.30257,     0.81736,     0.29738,     0.84691,     0.26881,     0.83697,     0.28051,     0.86974,     0.24885,    0.087339,    0.097156,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0150.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000093.jpg': [array([[          0,     0.73131,     0.22674,     0.73866,     0.19622,     0.74789,      0.1674,     0.76341,     0.24668,     0.77461,      0.2207,     0.69301,     0.22713,     0.69903,     0.20074,     0.71645,     0.27773,     0.72425,     0.25422,    0.081603,     0.11033,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0093.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000103.jpg': [array([[          0,     0.18557,     0.22695,     0.23689,     0.26143,     0.23479,     0.20846,     0.17511,     0.28168,     0.16839,     0.22513,     0.19788,     0.21555,     0.19336,     0.16104,     0.13437,     0.23131,     0.12501,     0.17286,     0.11188,     0.12064,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0103.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000153.jpg': [array([[          0,     0.63576,     0.39081,     0.63892,     0.37475,     0.66758,     0.37406,     0.62067,     0.35417,     0.64759,     0.35377,     0.62976,     0.43034,     0.65723,     0.42899,      0.6129,     0.40695,     0.63877,     0.40596,    0.054673,    0.076566,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0153.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000164.jpg': [array([[          0,     0.15743,      0.2497,     0.16235,     0.21291,     0.16605,     0.19207,      0.1091,     0.23901,     0.11533,      0.2168,     0.19959,     0.27762,      0.2014,     0.25557,     0.15084,     0.30202,     0.15477,     0.27878,    0.092302,     0.10995,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0164.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000056.jpg': [array([[          0,    0.089837,     0.57971,    0.093502,      0.6273,     0.12537,      0.6005,    0.049301,      0.5858,    0.083599,     0.55788,     0.10258,     0.59251,     0.13205,     0.56801,    0.062302,     0.55227,    0.093836,     0.52685,    0.082744,     0.10045,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0056.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000108.jpg': [array([[          0,     0.61102,     0.82468,     0.63214,     0.81569,     0.63894,     0.79938,     0.59125,     0.79453,     0.59978,     0.77984,     0.62498,     0.87049,     0.63208,     0.85156,     0.58465,     0.84742,     0.59344,     0.83028,    0.054284,    0.090653,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0108.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000035.jpg': [array([[          0,     0.33023,     0.54494,     0.35639,     0.52057,     0.33117,     0.49999,     0.34236,     0.54673,     0.31494,     0.52481,     0.33765,     0.55914,     0.31299,     0.53961,     0.32263,     0.58741,     0.29589,      0.5667,    0.060505,    0.087425,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0035.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000112.jpg': [array([[          0,     0.84593,     0.83437,     0.84468,     0.86582,     0.87015,     0.84285,      0.8127,     0.81279,     0.83591,     0.79038,     0.86203,     0.87364,     0.88672,     0.85181,     0.83025,     0.82224,     0.85281,      0.8009,     0.07402,    0.083261,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0112.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000094.jpg': [array([[          0,     0.10291,     0.49591,     0.12166,     0.50226,    0.087855,       0.495,     0.10811,     0.53287,    0.074829,     0.52709,      0.1222,      0.4637,     0.08981,     0.45519,     0.10916,     0.49427,    0.077245,     0.48721,     0.04737,    0.077685,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0094.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000119.jpg': [array([[          0,      0.2878,     0.85666,       0.331,     0.84452,     0.34335,     0.76354,     0.21746,     0.84976,     0.24364,     0.76591,     0.33638,     0.92958,     0.34715,     0.84613,     0.23358,     0.93675,     0.25583,      0.8504,     0.12969,     0.17321,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0119.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000014.jpg': [array([[          0,       0.435,     0.54823,     0.45969,     0.46004,       0.402,     0.44665,     0.40868,     0.58451,     0.34949,     0.56067,     0.49577,     0.53508,     0.44383,     0.51912,     0.45577,     0.64629,      0.4023,     0.62205,     0.14628,     0.19963,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0014.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000176.jpg': [array([[          0,     0.68536,    0.098997,     0.72377,    0.064137,     0.67295,    0.063619,      0.7011,     0.16355,     0.65535,      0.1631,     0.70455,    0.033095,     0.65816,    0.032618,     0.68544,     0.12749,     0.64331,     0.12707,    0.080457,     0.13094,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0176.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000189.jpg': [array([[          0,     0.14782,     0.32621,     0.16416,      0.3634,     0.17964,     0.34858,     0.12016,     0.34726,     0.13567,     0.33093,     0.16201,      0.3174,     0.17714,      0.3018,     0.11877,     0.29836,     0.13391,     0.28116,    0.060873,    0.082239,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0189.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000074.jpg': [array([[          0,     0.66454,     0.18376,     0.68108,      0.2149,     0.69203,     0.19193,      0.6441,     0.21485,     0.65307,      0.1918,     0.67733,     0.16984,      0.6879,     0.14483,      0.6409,     0.16964,     0.64955,     0.14453,     0.05113,    0.070364,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0074.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000025.jpg': [array([[          0,     0.80829,     0.69958,     0.78528,      0.6097,     0.80094,     0.62211,     0.87313,     0.67868,     0.90184,     0.70134,     0.73462,     0.70522,      0.7434,      0.7294,     0.81762,     0.77707,     0.83802,     0.81223,     0.16722,     0.20253,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0025.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000080.jpg': [array([[          0,     0.22263,      0.3676,     0.26978,     0.34512,     0.23706,     0.29877,     0.24965,     0.40003,     0.21251,     0.35333,     0.22236,     0.37049,     0.18796,     0.32752,     0.19913,     0.42531,     0.16014,     0.38229,     0.10964,     0.12654,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0080.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000126.jpg': [array([[          0,      0.1688,     0.87367,     0.16931,     0.89116,     0.20437,     0.88545,     0.14098,     0.92929,     0.17987,     0.92267,     0.16691,     0.82615,      0.2019,     0.82094,     0.13841,     0.85748,     0.17722,     0.85148,    0.065958,     0.10835,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0126.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000169.jpg': [array([[          0,     0.90744,    0.087898,     0.92945,     0.12633,     0.95941,     0.07575,      0.9076,     0.10708,     0.93822,    0.052918,     0.88623,      0.1064,     0.91265,    0.058183,     0.86364,    0.087391,     0.89047,    0.035947,    0.095775,    0.090382,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0169.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000012.jpg': [array([[          0,     0.67421,     0.71073,     0.70288,     0.69982,     0.68618,     0.67392,     0.67139,     0.70518,      0.6556,     0.67833,     0.68848,     0.73713,     0.67294,     0.71059,     0.65774,     0.74334,     0.64308,     0.71582,    0.059801,    0.069417,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0012.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000013.jpg': [array([[          0,     0.56735,     0.64911,     0.58371,     0.68211,     0.57784,     0.67518,     0.54805,     0.67115,     0.54076,     0.66395,      0.5917,     0.63181,     0.58636,     0.62277,     0.55668,     0.62267,     0.54996,     0.61351,    0.050937,    0.068592,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0013.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000178.jpg': [array([[          0,     0.36071,     0.66572,     0.36503,     0.71131,     0.36286,     0.72654,     0.32239,     0.66873,     0.31679,     0.68083,     0.40041,      0.6533,     0.40099,      0.6638,     0.35902,     0.61054,     0.35638,     0.61787,    0.084208,     0.11601,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0178.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000010.jpg': [array([[          0,     0.92146,     0.47658,     0.96117,     0.46305,     0.92222,     0.45296,     0.93853,      0.4371,     0.89932,     0.42819,     0.93415,     0.52412,     0.89719,     0.51056,     0.91056,     0.50122,     0.87349,     0.48848,     0.08768,    0.095934,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0010.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000059.jpg': [array([[          0,     0.60494,     0.81185,     0.56409,     0.86106,     0.58454,     0.83082,     0.63688,     0.88864,     0.65265,     0.85364,     0.56116,     0.76066,     0.58185,     0.73847,      0.6337,     0.77959,     0.64974,     0.75402,    0.091487,     0.15017,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0059.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000116.jpg': [array([[          0,     0.78556,     0.86632,     0.81568,     0.85134,     0.80489,     0.81867,     0.75866,     0.84946,     0.75172,     0.81786,     0.82004,     0.91181,     0.80917,     0.87592,     0.76391,     0.90813,     0.75676,     0.87349,    0.068317,    0.093947,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0116.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000066.jpg': [array([[          0,     0.85954,      0.4889,     0.89871,     0.41954,     0.87005,      0.4048,     0.80535,     0.47699,     0.78754,     0.45735,     0.93721,     0.52202,     0.90523,     0.49708,     0.84377,     0.57201,     0.82261,     0.54355,     0.14967,     0.16721,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0066.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000140.jpg': [array([[          0,     0.72522,     0.91475,     0.75221,     0.90426,     0.75521,     0.87417,      0.7535,     0.97232,     0.75681,     0.94186,     0.69744,     0.88463,     0.69867,     0.85516,      0.6941,     0.94814,     0.69532,     0.91822,    0.062706,     0.11716,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0140.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000123.jpg': [array([[          0,     0.27236,     0.15786,     0.24003,     0.14985,      0.2504,     0.12434,     0.26751,     0.13992,     0.27824,     0.11548,     0.26971,     0.19572,     0.28089,     0.17238,     0.29508,     0.18359,     0.30649,     0.16111,    0.066468,    0.080239,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0123.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000129.jpg': [array([[          0,     0.24246,     0.12651,     0.26554,    0.044149,     0.30075,     0.07302,      0.1753,    0.046477,     0.22007,    0.074503,     0.27659,     0.19132,      0.3088,     0.20252,     0.19115,     0.19037,     0.23198,     0.20146,      0.1335,     0.15837,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0129.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000132.jpg': [array([[          0,     0.64497,     0.82181,     0.66414,     0.81475,     0.66002,     0.78098,     0.62106,     0.81922,     0.61912,     0.78594,     0.67119,     0.85176,     0.66687,     0.81735,     0.62946,     0.85527,     0.62719,     0.82141,    0.052072,    0.074296,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0132.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000190.jpg': [array([[          0,     0.74989,     0.49791,     0.80519,     0.45931,     0.79553,     0.44897,     0.71653,     0.44494,     0.71536,      0.4364,     0.78692,     0.56547,     0.77928,     0.54578,     0.70256,      0.5462,     0.70265,     0.52911,     0.10264,     0.12907,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0190.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000142.jpg': [array([[          0,     0.81604,     0.41922,     0.81764,     0.36855,     0.76935,     0.36731,     0.86939,     0.37099,     0.81361,     0.36956,     0.80687,     0.46265,     0.75999,     0.46167,     0.85521,     0.47933,     0.80131,     0.47824,      0.1094,     0.11202,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0142.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000102.jpg': [array([[          0,     0.83258,     0.68541,     0.84641,     0.69226,     0.86239,     0.66848,     0.80668,     0.66571,     0.82228,     0.64371,     0.84696,     0.72392,     0.86229,     0.70063,     0.80873,     0.69657,     0.82371,     0.67498,    0.055705,    0.080209,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0102.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000117.jpg': [array([[          0,     0.45721,       0.341,      0.4909,      0.3004,     0.47147,     0.31336,      0.4352,     0.28951,     0.41859,     0.30403,      0.4919,     0.38279,     0.47244,     0.39015,     0.43643,     0.37728,     0.41977,     0.38548,    0.073305,     0.10064,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0117.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000023.jpg': [array([[          0,     0.15233,     0.29392,     0.17781,     0.32547,     0.18131,     0.29656,     0.13051,     0.31928,     0.13255,     0.28848,     0.17028,     0.29139,     0.17342,     0.26233,     0.12416,     0.28357,     0.12591,     0.25262,    0.057151,    0.072845,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0023.pngr', (640, 480)], '../data/dataset_transformed/drone/images/000147.jpg': [array([[          0,     0.80932,     0.46355,     0.83587,     0.47231,     0.80709,     0.42747,     0.86543,     0.44569,     0.83212,     0.39598,     0.78063,     0.51513,      0.7535,     0.46943,     0.80283,     0.49425,     0.77163,     0.44343,     0.11192,     0.11916,         320,         320,\n",
      "                640,         480,         320,         240,         640,         480]]), '../data/dataset_transformed/drone/mask/0147.pngr', (640, 480)], 'hash': 0, 'results': (160, 0, 0, 0, 0, 160, 0, 0, 160), 'version': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# print my cache\n",
    "cache_dic = torch.load(\"../data/dataset_transformed/drone/train.cache\")\n",
    "print(cache_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/LINEMOD/cat/train.cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/LINEMOD/cat/train.cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(cache)\n",
      "File \u001b[0;32m~/miniconda3/envs/yolov5/lib/python3.9/site-packages/torch/serialization.py:594\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    592\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/yolov5/lib/python3.9/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/yolov5/lib/python3.9/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/LINEMOD/cat/train.cache'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cache = torch.load(r\"../data/LINEMOD/cat/train.cache\")\n",
    "print(cache)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/dataset_transformed/drone/test.txt\n",
      "../data/dataset_transformed/drone/train.txt\n",
      "../data/dataset_transformed/drone/training_range.txt\n",
      "../data/dataset_transformed/drone/validation.txt\n",
      "../data/dataset_transformed/drone/test.cache\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/data_curation/convert_drone_dataset.py\", line 59, in <module>\n",
      "    fix_image_paths(args.path_to_dataset)\n",
      "  File \"/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/data_curation/convert_drone_dataset.py\", line 37, in fix_image_paths\n",
      "    lines = f.readlines()\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "# Be sure to convert the data to our format focusing on the camera intrinsics\n",
    "!python data_curation/convert_drone_dataset.py --path_to_dataset ../data/dataset_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv5 77d2023 torch 1.9.1+cu102 CPU\n",
      "\n",
      "Namespace(weights='../data/weights/yolov5s.pt', cfg='models/yolov5s_6dpose_bifpn.yaml', data='configs/linemod/cat.yaml', hyp='configs/hyp.single.yaml', epochs=1, batch_size=2, img_size=[640, 640], rect=True, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='', single_cls=False, optimizer='Adam', sync_bn=False, local_rank=-1, log_imgs=8, log_artifacts=False, workers=8, project='runs/train', entity=None, name='exp', exist_ok=False, quad=False, linear_lr=False, standard_lr=False, symetric=False, world_size=1, global_rank=-1, save_dir='runs/train/exp40', total_batch_size=2)\n",
      "Start Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0001, lrf=0.01, lr_factor=0.5, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=1.5, obj=0.1, obj_pw=1.0, cls=0.5, cls_pw=1.0, anchor_t=4.0, hsv_h=0.03, hsv_s=0.7, hsv_v=0.4, degrees=30.0, translate=0.2, scale=0.2, shear=0, perspective=0, blur=0, blur_kernel=5, flipud=False, fliplr=False, background=0.5, multi_scale=False, occlude=0.3, theta=0.2, alpha=2, pretrain_epochs=40\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19       [-1, 14, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1   1842356  models.yolo.Pose                        [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model Summary: 316 layers, 8914036 parameters, 8914036 gradients, 19.7 GFLOPS\n",
      "\n",
      "Transferred 340/548 items from ../data/weights/yolov5s.pt\n",
      "Transferred 340/548 items from ../data/weights/yolov5s.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "Optimizer groups: 93 .bias, 93 conv.weight, 90 other\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvitaondr\u001b[0m (\u001b[33mvitaondr-czech-technical-university-in-prague\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/wandb/run-20250507_170403-s0nlcr7d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexp40\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/vitaondr-czech-technical-university-in-prague/YOLOv5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/vitaondr-czech-technical-university-in-prague/YOLOv5/runs/s0nlcr7d\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../data/LINEMOD/cat/train.cache' for images, labels, masks... 1\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../data/LINEMOD/cat/test.cache' for images, labels, masks... 1002\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 0.00, Best Possible Recall (BPR) = 0.0000. Attempting to improve anchors, please wait...\n",
      "<utils.datasets.LoadImagesAndLabelsPose object at 0x7f69d98ad2b0>\n",
      "[        640         480]\n",
      "[[    0.12656     0.16723]]\n",
      "[     80.997      80.268]\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 177 points...\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 9.00 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.722/0.924-mean/best, past_thr=0.722-mean: 60,94,  86,79,  77,105,  98,96,  81,138,  116,104,  103,125,  128,128,  126,152\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.9249: 100%|| 1\u001b[0m\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 9.00 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.724/0.925-mean/best, past_thr=0.724-mean: 58,94,  87,81,  76,105,  97,97,  80,137,  113,106,  101,122,  127,127,  128,148\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
      "\n",
      "Image sizes 640 train, 640 test\n",
      "Using 2 dataloader workers\n",
      "Logging results to runs/train/exp40\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "     Epoch   gpu_mem     l_obj     l_box     l_cls n_targets  img_size\n",
      "       0/0        0G     2.166     1.057         0         2       640:   3%| | \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/train.py\", line 547, in <module>\n",
      "    train(hyp, opt, device, tb_writer, wandb)\n",
      "  File \"/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/train.py\", line 325, in train\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/_tensor.py\", line 255, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 147, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/wandb/integration/torch/wandb_torch.py\", line 276, in <lambda>\n",
      "    handle = var.register_hook(lambda grad: _callback(grad, log_track))\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/wandb/integration/torch/wandb_torch.py\", line 274, in _callback\n",
      "    self.log_tensor_stats(grad.data, name)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/wandb/integration/torch/wandb_torch.py\", line 254, in log_tensor_stats\n",
      "    wandb.run._log(\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 1587, in _log\n",
      "    self._partial_history_callback(data, step, commit)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 1417, in _partial_history_callback\n",
      "    self._backend.interface.publish_partial_history(\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/wandb/sdk/interface/interface.py\", line 680, in publish_partial_history\n",
      "    data[\"_timestamp\"] = time.time()\n",
      "KeyboardInterrupt\n",
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# By default, the training script will download the COCO pretrained YOLOv5-v7 model. If this is no longer supported, you can download the weights from the YOLOv5-6D google drive (https://drive.google.com/drive/folders/11BW41xO3R1UBnc2Dx1xA3CPbYPGTrfHQ?usp=drive_link) and place them in the weights folder\n",
    "!python train.py --batch 2 --epochs 1 --cfg models/yolov5s_6dpose_bifpn.yaml --hyp configs/hyp.single.yaml --weights ../data/weights/yolov5s.pt --data configs/linemod/cat.yaml --rect --optimizer Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv5 7b4e07f torch 1.9.1+cu102 CPU\n",
      "\n",
      "Namespace(weights='../data/weights/yolov5s.pt', cfg='models/yolov5s_6dpose_bifpn.yaml', data='configs/drone_data/drone.yaml', hyp='configs/hyp.single.yaml', epochs=1, batch_size=2, img_size=[640, 640], rect=True, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='', single_cls=False, optimizer='Adam', sync_bn=False, local_rank=-1, log_imgs=8, log_artifacts=False, workers=8, project='runs/train', entity=None, name='exp', exist_ok=False, quad=False, linear_lr=False, standard_lr=False, symetric=False, world_size=1, global_rank=-1, save_dir='runs/train/exp21', total_batch_size=2)\n",
      "Start Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0001, lrf=0.01, lr_factor=0.5, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=1.5, obj=0.1, obj_pw=1.0, cls=0.5, cls_pw=1.0, anchor_t=4.0, hsv_h=0.03, hsv_s=0.7, hsv_v=0.4, degrees=30.0, translate=0.2, scale=0.2, shear=0, perspective=0, blur=0, blur_kernel=5, flipud=False, fliplr=False, background=0.5, multi_scale=False, occlude=0.3, theta=0.2, alpha=2, pretrain_epochs=40\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19       [-1, 14, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1   1842356  models.yolo.Pose                        [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model Summary: 316 layers, 8914036 parameters, 8914036 gradients, 19.7 GFLOPS\n",
      "\n",
      "Transferred 340/548 items from ../data/weights/yolov5s.pt\n",
      "Transferred 340/548 items from ../data/weights/yolov5s.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "Optimizer groups: 93 .bias, 93 conv.weight, 90 other\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvitaondr\u001b[0m (\u001b[33mvitaondr-czech-technical-university-in-prague\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/wandb/run-20250507_180600-5ucmlkym\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexp21\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/vitaondr-czech-technical-university-in-prague/YOLOv5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/vitaondr-czech-technical-university-in-prague/YOLOv5/runs/5ucmlkym\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../data/dataset_transformed/drone/train.cache' for images, labe\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../data/dataset_transformed/drone/validation.cache' for images, l\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 0.62, Best Possible Recall (BPR) = 0.1937. Attempting to improve anchors, please wait...\n",
      "<utils.datasets.LoadImagesAndLabelsPose object at 0x7fe83c56c040>\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 320 points...\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.99 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.406/0.836-mean/best, past_thr=0.539-mean: 12,12,  20,20,  29,26,  37,39,  53,49,  64,64,  100,89,  152,140,  254,281\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.8372: 100%|| 1\u001b[0m\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.99 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.406/0.837-mean/best, past_thr=0.539-mean: 12,12,  20,20,  29,27,  37,39,  53,47,  63,63,  100,91,  145,141,  255,287\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
      "\n",
      "Image sizes 640 train, 640 test\n",
      "Using 2 dataloader workers\n",
      "Logging results to runs/train/exp21\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "     Epoch   gpu_mem     l_obj     l_box     l_cls n_targets  img_size\n",
      "       0/0        0G     2.115     2.273         0         2       640: 100%|| \n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.60694, 0.49937, 0.60419, 0.46622, 0.59467, 0.46499, 0.58605, 0.51102, 0.57744, 0.50754, 0.63594, 0.49076, 0.62499, 0.48833, 0.61808, 0.53491, 0.60801, 0.53030, 0.05850, 0.06992]])\n",
      "tensor([[0.00000, 0.12361, 0.31227, 0.12049, 0.29577, 0.11950, 0.28472, 0.09969, 0.32365, 0.09901, 0.31232, 0.14803, 0.31543, 0.14670, 0.30458, 0.12783, 0.34190, 0.12679, 0.33080, 0.04902, 0.05718]])\n",
      "  5%|                                         | 1/20 [00:00<00:13,  1.45it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.49491, 0.47239, 0.48936, 0.44065, 0.49041, 0.43412, 0.52134, 0.47657, 0.52362, 0.47139, 0.46666, 0.47481, 0.46685, 0.46952, 0.49894, 0.51132, 0.50038, 0.50742, 0.05696, 0.07720]])\n",
      "tensor([[0.00000, 0.75630, 0.41942, 0.75462, 0.40940, 0.74889, 0.41035, 0.76621, 0.41116, 0.76034, 0.41215, 0.75408, 0.42611, 0.74838, 0.42711, 0.76558, 0.42835, 0.75975, 0.42938, 0.01784, 0.01998]])\n",
      " 10%|                                       | 2/20 [00:01<00:10,  1.68it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.33678, 0.59380, 0.33707, 0.62007, 0.35067, 0.61177, 0.32698, 0.61041, 0.34149, 0.60164, 0.32843, 0.58897, 0.34183, 0.58096, 0.31799, 0.57756, 0.33227, 0.56912, 0.03267, 0.05095]])\n",
      "tensor([[0.00000, 0.03368, 0.21031, 0.00775, 0.18932, 0.01560, 0.18318, 0.01827, 0.23112, 0.02563, 0.22440, 0.03984, 0.19751, 0.04693, 0.19130, 0.04901, 0.23839, 0.05567, 0.23163, 0.04792, 0.05521]])\n",
      " 15%|                                     | 3/20 [00:01<00:09,  1.76it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.52861, 0.64699, 0.53799, 0.63013, 0.53175, 0.63183, 0.54268, 0.65677, 0.53653, 0.65927, 0.52037, 0.63560, 0.51373, 0.63742, 0.52483, 0.66186, 0.51827, 0.66446, 0.02895, 0.03433]])\n",
      "tensor([[0.00000, 0.22680, 0.75571, 0.23993, 0.78422, 0.28210, 0.69398, 0.14864, 0.74979, 0.19982, 0.66425, 0.25887, 0.83697, 0.29480, 0.75128, 0.17786, 0.80588, 0.22096, 0.72402, 0.14616, 0.17273]])\n",
      " 20%|                                   | 4/20 [00:02<00:08,  1.84it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.59395, 0.04852, 0.60992, 0.04898, 0.59961, 0.02346, 0.58869, 0.07255, 0.57823, 0.04799, 0.60796, 0.04617, 0.59800, 0.02157, 0.58754, 0.06894, 0.57744, 0.04523, 0.03248, 0.05098]])\n",
      "tensor([[0.00000, 0.71396, 0.11440, 0.71789, 0.09630, 0.72204, 0.09798, 0.70138, 0.11085, 0.70557, 0.11236, 0.72142, 0.11602, 0.72554, 0.11752, 0.70482, 0.13001, 0.70898, 0.13134, 0.02416, 0.03504]])\n",
      " 25%|                                 | 5/20 [00:02<00:06,  2.35it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.21470, 0.56065, 0.21993, 0.57875, 0.23804, 0.56655, 0.20936, 0.58481, 0.22854, 0.57189, 0.20539, 0.54624, 0.22369, 0.53413, 0.19391, 0.55050, 0.21332, 0.53769, 0.04412, 0.05068]])\n",
      "tensor([[0.00000, 0.60009, 0.33303, 0.62114, 0.34397, 0.62571, 0.32591, 0.58052, 0.32595, 0.58521, 0.30804, 0.61212, 0.36315, 0.61636, 0.34643, 0.57469, 0.34644, 0.57904, 0.32985, 0.05101, 0.05511]])\n",
      " 30%|                              | 6/20 [00:02<00:05,  2.79it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.36719, 0.53948, 0.35000, 0.55952, 0.35998, 0.55341, 0.37589, 0.56244, 0.38550, 0.55608, 0.34574, 0.52452, 0.35595, 0.51922, 0.37183, 0.52640, 0.38167, 0.52090, 0.03977, 0.04322]])\n",
      "tensor([[0.00000, 0.76258, 0.83464, 0.78881, 0.83404, 0.79072, 0.83337, 0.76828, 0.87259, 0.76977, 0.87273, 0.75518, 0.79917, 0.75633, 0.79781, 0.73507, 0.83594, 0.73583, 0.83532, 0.05564, 0.07492]])\n",
      " 35%|                            | 7/20 [00:02<00:04,  3.25it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.23867, 0.69810, 0.30045, 0.66009, 0.26220, 0.63224, 0.23436, 0.66354, 0.19444, 0.63147, 0.27528, 0.75616, 0.23805, 0.72319, 0.20474, 0.77449, 0.16623, 0.73565, 0.13422, 0.14302]])\n",
      "tensor([[0.00000, 0.07934, 0.36620, 0.04558, 0.36130, 0.04435, 0.35324, 0.07298, 0.40201, 0.07142, 0.39379, 0.08853, 0.34058, 0.08670, 0.33305, 0.11292, 0.38132, 0.11083, 0.37361, 0.06856, 0.06896]])\n",
      " 40%|                          | 8/20 [00:03<00:03,  3.60it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.28571, 0.62805, 0.31372, 0.69332, 0.33268, 0.66475, 0.26922, 0.65298, 0.29161, 0.62153, 0.27305, 0.64380, 0.29139, 0.61809, 0.22712, 0.60060, 0.24857, 0.57264, 0.10557, 0.12067]])\n",
      "tensor([[0.00000, 0.72492, 0.54889, 0.71709, 0.56332, 0.71989, 0.55463, 0.71210, 0.54435, 0.71475, 0.53571, 0.73443, 0.56498, 0.73744, 0.55630, 0.72890, 0.54594, 0.73176, 0.53730, 0.02535, 0.02927]])\n",
      " 45%|                        | 9/20 [00:03<00:02,  3.88it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.52620, 0.15324, 0.53156, 0.15024, 0.52949, 0.14179, 0.52192, 0.15470, 0.51987, 0.14625, 0.53195, 0.15835, 0.52993, 0.15006, 0.52250, 0.16270, 0.52050, 0.15442, 0.01207, 0.02091]])\n",
      "tensor([[0.00000, 0.87506, 0.21395, 0.86890, 0.18262, 0.91538, 0.16248, 0.83903, 0.19152, 0.88079, 0.17337, 0.87428, 0.25454, 0.91950, 0.23601, 0.84470, 0.25679, 0.88544, 0.23997, 0.08047, 0.09431]])\n",
      " 50%|                     | 10/20 [00:03<00:02,  4.12it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.76368, 0.25611, 0.77822, 0.24191, 0.76247, 0.24070, 0.76770, 0.24284, 0.75187, 0.24158, 0.77348, 0.27024, 0.75810, 0.26841, 0.76304, 0.27177, 0.74758, 0.26987, 0.03063, 0.03107]])\n",
      "tensor([[0.00000, 0.25791, 0.83875, 0.26752, 0.85036, 0.24518, 0.86106, 0.27102, 0.85298, 0.24964, 0.86328, 0.26366, 0.81531, 0.24172, 0.82524, 0.26724, 0.81925, 0.24623, 0.82885, 0.02930, 0.04797]])\n",
      " 55%|                   | 11/20 [00:03<00:02,  4.27it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.04136, 0.46271, 0.00938, 0.47198, 0.00955, 0.45627, 0.03747, 0.47525, 0.03760, 0.46052, 0.04550, 0.47140, 0.04564, 0.45578, 0.07118, 0.47469, 0.07129, 0.46004, 0.06190, 0.01947]])\n",
      "tensor([[0.00000, 0.47606, 0.44015, 0.47080, 0.46363, 0.46233, 0.47170, 0.49969, 0.45178, 0.49032, 0.46008, 0.45839, 0.41999, 0.45047, 0.42982, 0.48755, 0.40954, 0.47870, 0.41949, 0.04922, 0.06216]])\n",
      " 60%|                 | 12/20 [00:03<00:01,  4.58it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.51454, 0.30261, 0.51752, 0.30282, 0.51052, 0.28922, 0.53525, 0.29891, 0.52816, 0.28460, 0.50363, 0.32382, 0.49635, 0.31032, 0.52094, 0.32088, 0.51355, 0.30667, 0.03890, 0.03922]])\n",
      "tensor([[0.00000, 0.11088, 0.56701, 0.05097, 0.58819, 0.06520, 0.51686, 0.08153, 0.59917, 0.09341, 0.53532, 0.13174, 0.59389, 0.14312, 0.52518, 0.15250, 0.60387, 0.16215, 0.54213, 0.11118, 0.08702]])\n",
      " 65%|               | 13/20 [00:04<00:01,  4.87it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[ 0.00000,  0.00537,  0.16343,  0.02477,  0.12532,  0.03320,  0.12683, -0.04769,  0.12130, -0.03424,  0.12315,  0.04492,  0.20236,  0.05182,  0.19935, -0.02309,  0.20235, -0.01174,  0.19920,  0.09951,  0.08106]])\n",
      "tensor([[0.00000, 0.08041, 0.43545, 0.08378, 0.44862, 0.07019, 0.44355, 0.08089, 0.44534, 0.06753, 0.44032, 0.09033, 0.42889, 0.07690, 0.42350, 0.08732, 0.42601, 0.07411, 0.42069, 0.02280, 0.02793]])\n",
      " 70%|             | 14/20 [00:04<00:01,  4.86it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.47148, 0.45415, 0.45824, 0.43175, 0.46899, 0.42555, 0.45523, 0.44644, 0.46533, 0.44052, 0.47379, 0.47175, 0.48454, 0.46523, 0.46978, 0.48408, 0.47989, 0.47789, 0.02932, 0.05853]])\n",
      "tensor([[0.00000, 0.96440, 0.75960, 0.97596, 0.76996, 0.97420, 0.77048, 0.95784, 0.76839, 0.95590, 0.76888, 0.97239, 0.74995, 0.97060, 0.75022, 0.95448, 0.74877, 0.95252, 0.74903, 0.02345, 0.02171]])\n",
      " 75%|          | 15/20 [00:04<00:00,  5.14it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.11201, 0.61413, 0.09442, 0.61019, 0.10497, 0.60865, 0.10368, 0.63308, 0.11400, 0.63092, 0.11282, 0.59622, 0.12285, 0.59507, 0.12209, 0.61903, 0.13189, 0.61725, 0.03747, 0.03801]])\n",
      "tensor([[0.00000, 0.22960, 0.64040, 0.20237, 0.67660, 0.21833, 0.65861, 0.24970, 0.67490, 0.26277, 0.65711, 0.19705, 0.61679, 0.21360, 0.60213, 0.24524, 0.61538, 0.25879, 0.60089, 0.06573, 0.07571]])\n",
      " 80%|        | 16/20 [00:04<00:00,  5.07it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.01364, 0.13938, 0.02304, 0.15082, 0.02569, 0.14249, 0.00231, 0.13702, 0.00505, 0.12850, 0.02115, 0.15228, 0.02377, 0.14408, 0.00072, 0.13875, 0.00341, 0.13036, 0.02498, 0.02378]])\n",
      "tensor([[0.00000, 0.22541, 0.65650, 0.19967, 0.68776, 0.20352, 0.67769, 0.20069, 0.63972, 0.20438, 0.62944, 0.24467, 0.68710, 0.24898, 0.67753, 0.24365, 0.64138, 0.24775, 0.63161, 0.04931, 0.05832]])\n",
      " 85%|      | 17/20 [00:04<00:00,  5.31it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.40237, 0.93000, 0.45663, 0.86364, 0.47738, 0.90205, 0.38684, 0.99057, 0.40254, 1.04287, 0.39762, 0.81118, 0.41468, 0.84804, 0.31835, 0.94557, 0.32870, 0.99825, 0.15903, 0.23169]])\n",
      "tensor([[0.00000, 0.69739, 0.48835, 0.70485, 0.48159, 0.69768, 0.47667, 0.70124, 0.49437, 0.69403, 0.48941, 0.69887, 0.48631, 0.69187, 0.48149, 0.69531, 0.49884, 0.68825, 0.49398, 0.01660, 0.02217]])\n",
      " 90%|    | 18/20 [00:05<00:00,  5.20it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.43260, 0.29080, 0.42181, 0.26822, 0.43033, 0.26728, 0.43228, 0.28074, 0.44044, 0.27990, 0.42092, 0.30272, 0.42945, 0.30196, 0.43145, 0.31360, 0.43962, 0.31293, 0.01952, 0.04632]])\n",
      "tensor([[ 0.00000e+00,  2.30043e-02,  2.94618e-01,  5.30616e-02,  3.25629e-01,  4.77124e-02,  3.24951e-01,  4.63564e-02,  2.68396e-01,  4.06911e-02,  2.65831e-01,  7.56208e-03,  3.23490e-01,  5.15440e-04,  3.22718e-01, -1.90848e-04,  2.64285e-01, -7.63178e-03,  2.61492e-01,  6.06934e-02,  6.41374e-02]])\n",
      " 95%|  | 19/20 [00:05<00:00,  5.40it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[ 0.00000,  0.08215,  0.32781,  0.02812,  0.40429, -0.00337,  0.27929,  0.02282,  0.36299, -0.00480,  0.25178,  0.16055,  0.38603,  0.14350,  0.27627,  0.14098,  0.35131,  0.12463,  0.25235,  0.16535,  0.15251]])\n",
      "tensor([[0.00000, 0.16666, 0.36641, 0.20232, 0.37824, 0.20049, 0.36362, 0.16774, 0.33331, 0.16750, 0.32008, 0.16579, 0.41114, 0.16566, 0.39482, 0.12859, 0.36767, 0.13031, 0.35258, 0.07373, 0.09106]])\n",
      "100%|| 20/20 [00:05<00:00,  3.68it/s]\n",
      "-----------------------------------\n",
      "  tensor to cuda : 0.001413\n",
      "         predict : 0.170301\n",
      "    compute loss : 0.004524\n",
      "get_region_boxes : 0.004384\n",
      "            eval : 0.004945\n",
      "             pnp : 0.000540\n",
      "           total : 0.180622\n",
      "-----------------------------------\n",
      "   Mean corner error is 283.968262\n",
      "   Acc using 5 px 2D Projection = 0.00%\n",
      "   Acc using 0.020000000000000004 vx 3D Transformation = 0.00%\n",
      "   Acc using 5 cm 5 degree metric = 0.00%\n",
      "   Translation error: 12.449777, angle error: 115.452264\n",
      "(283.96826, 0.0, 0.0, 0.0, 2.1443676948547363, 2.377977132797241, 0.0, 95.11908721923828)\n",
      "Saving best model with fitness: [          0]\n",
      "Optimizer stripped from runs/train/exp21/weights/last.pt, 18.1MB\n",
      "Optimizer stripped from runs/train/exp21/weights/best.pt, 18.1MB\n",
      "1 epochs completed in 0.042 hours.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m\u001b[0m uploading wandb-summary.json 447.2KB/447.2KB (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m\u001b[0m uploading wandb-summary.json 447.2KB/447.2KB (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m\u001b[0m uploading wandb-summary.json 447.2KB/447.2KB (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m\u001b[0m uploading wandb-summary.json 447.2KB/447.2KB (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m\u001b[0m uploading wandb-summary.json 447.2KB/447.2KB (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m\u001b[0m uploading wandb-summary.json 447.2KB/447.2KB (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                val/acc \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val/acc10cm10deg \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val/acc3d \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val/mean_corner_err_2d \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/total_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 2.27333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 2.11469\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                val/acc 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val/acc10cm10deg 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val/acc3d 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 2.37798\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val/mean_corner_err_2d 283.96826\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 2.14437\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/total_loss 95.11909\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run \u001b[33mexp21\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/vitaondr-czech-technical-university-in-prague/YOLOv5/runs/5ucmlkym\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at: \u001b[34m\u001b[4mhttps://wandb.ai/vitaondr-czech-technical-university-in-prague/YOLOv5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 9 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250507_180600-5ucmlkym/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# By default, the training script will download the COCO pretrained YOLOv5-v7 model. If this is no longer supported, you can download the weights from the YOLOv5-6D google drive (https://drive.google.com/drive/folders/11BW41xO3R1UBnc2Dx1xA3CPbYPGTrfHQ?usp=drive_link) and place them in the weights folder\n",
    "!python train.py --batch 2 --epochs 1 --cfg models/yolov5s_6dpose_bifpn.yaml --hyp configs/hyp.single.yaml --weights ../data/weights/yolov5s.pt --data configs/drone_data/drone.yaml --rect --optimizer Adam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is how to play the training with sudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for vitaondr: \n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!sudo env \"PATH=$PATH\" python train.py --batch 4 --epochs 100 --cfg models/yolov5s_6dpose_bifpn.yaml --hyp configs/hyp.single.yaml --weights ../data/weights/yolov5s.pt --data configs/drone_data/drone.yaml --rect --optimizer Adam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real dataset train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv5 3a745a6 torch 1.9.1+cu102 CPU\n",
      "\n",
      "Namespace(weights='../data/weights/yolov5s.pt', cfg='models/yolov5s_6dpose_bifpn.yaml', data='configs/drone_data/drone_real.yaml', hyp='configs/hyp.single.yaml', epochs=1, batch_size=2, img_size=[640, 640], rect=True, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='', single_cls=False, optimizer='Adam', sync_bn=False, local_rank=-1, log_imgs=8, log_artifacts=False, workers=8, project='runs/train', entity=None, name='exp', exist_ok=False, quad=False, linear_lr=False, standard_lr=False, symetric=False, world_size=1, global_rank=-1, save_dir='runs/train/exp22', total_batch_size=2)\n",
      "Start Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0001, lrf=0.01, lr_factor=0.5, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=1.5, obj=0.1, obj_pw=1.0, cls=0.5, cls_pw=1.0, anchor_t=4.0, hsv_h=0.03, hsv_s=0.7, hsv_v=0.4, degrees=30.0, translate=0.2, scale=0.2, shear=0, perspective=0, blur=0, blur_kernel=5, flipud=False, fliplr=False, background=0.5, multi_scale=False, occlude=0.3, theta=0.2, alpha=2, pretrain_epochs=40\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19       [-1, 14, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1   1842356  models.yolo.Pose                        [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
      "Model Summary: 316 layers, 8914036 parameters, 8914036 gradients, 19.7 GFLOPS\n",
      "\n",
      "Transferred 340/548 items from ../data/weights/yolov5s.pt\n",
      "Transferred 340/548 items from ../data/weights/yolov5s.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "Optimizer groups: 93 .bias, 93 conv.weight, 90 other\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvitaondr\u001b[0m (\u001b[33mvitaondr-czech-technical-university-in-prague\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/wandb/run-20250506_181331-69p1u0pc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexp22\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/vitaondr-czech-technical-university-in-prague/YOLOv5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/vitaondr-czech-technical-university-in-prague/YOLOv5/runs/69p1u0pc\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../data/real_dataset/drone/train.cache' for images, labels, mas\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../data/real_dataset/drone/test.cache' for images, labels, masks.\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 0.81, Best Possible Recall (BPR) = 0.2193. Attempting to improve anchors, please wait...\n",
      "<utils.datasets.LoadImagesAndLabelsPose object at 0x7ff1769b4040>\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 16654 points...\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 7.42 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.496/0.901-mean/best, past_thr=0.564-mean: 8,8,  13,13,  18,17,  18,22,  28,25,  35,35,  41,41,  57,65,  101,82\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.9037: 100%|| 1\u001b[0m\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 7.42 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.492/0.904-mean/best, past_thr=0.561-mean: 7,7,  13,13,  17,17,  17,22,  27,24,  35,36,  42,42,  55,66,  100,81\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
      "\n",
      "Image sizes 640 train, 640 test\n",
      "Using 2 dataloader workers\n",
      "Logging results to runs/train/exp22\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "     Epoch   gpu_mem     l_obj     l_box     l_cls n_targets  img_size\n",
      "       0/0        0G     1.955     1.446         0         2       640:   6%| | \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/train.py\", line 547, in <module>\n",
      "    train(hyp, opt, device, tb_writer, wandb)\n",
      "  File \"/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/train.py\", line 325, in train\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/_tensor.py\", line 255, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 147, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/wandb/integration/torch/wandb_torch.py\", line 276, in <lambda>\n",
      "    handle = var.register_hook(lambda grad: _callback(grad, log_track))\n",
      "KeyboardInterrupt\n",
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python train.py --batch 2 --epochs 1 --cfg models/yolov5s_6dpose_bifpn.yaml --hyp configs/hyp.single.yaml --weights ../data/weights/yolov5s.pt --data configs/drone_data/drone_real.yaml --rect --optimizer Adam #--img-size 1280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights=['../data/LINEMOD/cat/weights/best.pt'], source='../data/LINEMOD/cat/JPEGImages/000000.jpg', static_camera='configs/linemod/linemod_camera.json', mesh_data='../data/LINEMOD/cat/cat.ply', img_size=640, conf_thres=0.25, device='', view_img=False, name='exp', exist_ok=False, project='runs/detect')\n",
      "YOLOv5 3a745a6 torch 1.9.1+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 391 layers, 97574820 parameters, 0 gradients, 225.4 GFLOPS\n",
      "[[  -0.033737   -0.033737   -0.033737   -0.033737    0.033274    0.033274    0.033274    0.033274]\n",
      " [  -0.061801   -0.061801    0.065832    0.065832   -0.061801   -0.061801    0.065832    0.065832]\n",
      " [  -0.045446    0.072011   -0.045446    0.072011   -0.045446    0.072011   -0.045446    0.072011]\n",
      " [          1           1           1           1           1           1           1           1]]\n",
      "image 1/1 /home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/../data/LINEMOD/cat/JPEGImages/000000.jpg: tensor([[364.15668, 161.39651, 332.66818, 173.51559, 323.54919, 125.69576, 398.54477, 161.72449, 394.68384, 111.85746, 335.92682, 197.77258, 326.57761, 149.67729, 404.42773, 185.40781, 401.13223, 135.72612,   0.85526,   0.00000]])\n",
      "480x640 1 cat, Done. (0.610s)\n",
      "Results saved to runs/detect/exp18\n",
      "1 labels saved to runs/detect/exp18/labels\n",
      "Done. (1.068s)\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights ../data/LINEMOD/cat/weights/best.pt --img 640 --conf 0.25 --source ../data/LINEMOD/cat/JPEGImages/000000.jpg --static-camera configs/linemod/linemod_camera.json --mesh-data ../data/LINEMOD/cat/cat.ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights=['runs/train/exp19/weights/best.pt'], source='/home/ondra/BC/MRS_simulation/mrs-flight-forge-connector-fork/dataset_transformed/drone/images/000179.jpg', static_camera='configs/drone_data/drone_camera.json', mesh_data='../data/dataset_transformed/drone/drone.ply', img_size=640, conf_thres=0.25, device='', view_img=False, name='exp', exist_ok=False, project='runs/detect')\n",
      "YOLOv5 3a745a6 torch 1.9.1+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 226 layers, 8894676 parameters, 0 gradients, 19.4 GFLOPS\n",
      "image 1/1 /home/ondra/BC/MRS_simulation/mrs-flight-forge-connector-fork/dataset_transformed/drone/images/000179.jpg: tensor([[150.47800, 315.88943, 165.93604, 341.61694, 139.29539, 327.27637, 136.67024, 342.94180, 110.52475, 330.38013, 185.84938, 298.49649, 161.24179, 281.97772, 155.70067, 303.04956, 131.86714, 288.33240,   0.89323,   0.00000]])\n",
      "480x640 1 drone, Done. (0.091s)\n",
      "Results saved to runs/detect/exp15\n",
      "1 labels saved to runs/detect/exp15/labels\n",
      "Done. (0.558s)\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights runs/train/exp19/weights/best.pt --img 640 --conf 0.25 --source /home/ondra/BC/MRS_simulation/mrs-flight-forge-connector-fork/dataset_transformed/drone/images/000179.jpg --static-camera configs/drone_data/drone_camera.json --mesh-data ../data/dataset_transformed/drone/drone.ply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights=['runs/train/exp19/weights/best.pt'], data='configs/drone_data/drone.yaml', batch_size=1, img_size=640, conf_thres=0.01, num_keypoints=9, task='val', device='', single_cls=True, augment=False, verbose=False, save_txt=False, save_hybrid=True, save_conf=True, save_json=True, project='runs/test', name='exp', exist_ok=False, symetric=False, test_plotting=True)\n",
      "YOLOv5 7b4e07f torch 1.9.1+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 226 layers, 8894676 parameters, 0 gradients, 19.4 GFLOPS\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../data/dataset_transformed/drone/validation.cache' for images, l\u001b[0m\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]target:  tensor([[0.00000, 0.00000, 0.60694, 0.49937, 0.60419, 0.46622, 0.59467, 0.46499, 0.58605, 0.51102, 0.57744, 0.50754, 0.63594, 0.49076, 0.62499, 0.48833, 0.61808, 0.53491, 0.60801, 0.53030, 0.05850, 0.06992]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.60694, 0.49937, 0.60419, 0.46622, 0.59467, 0.46499, 0.58605, 0.51102, 0.57744, 0.50754, 0.63594, 0.49076, 0.62499, 0.48833, 0.61808, 0.53491, 0.60801, 0.53030, 0.05850, 0.06992]])\n",
      "  2%|                                           | 1/40 [00:00<00:09,  4.02it/s]target:  tensor([[0.00000, 0.00000, 0.12361, 0.31227, 0.12049, 0.29577, 0.11950, 0.28472, 0.09969, 0.32365, 0.09901, 0.31232, 0.14803, 0.31543, 0.14670, 0.30458, 0.12783, 0.34190, 0.12679, 0.33080, 0.04902, 0.05718]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.12361, 0.31227, 0.12049, 0.29577, 0.11950, 0.28472, 0.09969, 0.32365, 0.09901, 0.31232, 0.14803, 0.31543, 0.14670, 0.30458, 0.12783, 0.34190, 0.12679, 0.33080, 0.04902, 0.05718]])\n",
      "  5%|                                         | 2/40 [00:00<00:08,  4.57it/s]target:  tensor([[0.00000, 0.00000, 0.49491, 0.47239, 0.48936, 0.44065, 0.49041, 0.43412, 0.52134, 0.47657, 0.52362, 0.47139, 0.46666, 0.47481, 0.46685, 0.46952, 0.49894, 0.51132, 0.50038, 0.50742, 0.05696, 0.07720]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.49491, 0.47239, 0.48936, 0.44065, 0.49041, 0.43412, 0.52134, 0.47657, 0.52362, 0.47139, 0.46666, 0.47481, 0.46685, 0.46952, 0.49894, 0.51132, 0.50038, 0.50742, 0.05696, 0.07720]])\n",
      "  8%|                                        | 3/40 [00:00<00:07,  5.14it/s]target:  tensor([[0.00000, 0.00000, 0.75630, 0.41942, 0.75462, 0.40940, 0.74889, 0.41035, 0.76621, 0.41116, 0.76034, 0.41215, 0.75408, 0.42611, 0.74838, 0.42711, 0.76558, 0.42835, 0.75975, 0.42938, 0.01784, 0.01998]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.75630, 0.41942, 0.75462, 0.40940, 0.74889, 0.41035, 0.76621, 0.41116, 0.76034, 0.41215, 0.75408, 0.42611, 0.74838, 0.42711, 0.76558, 0.42835, 0.75975, 0.42938, 0.01784, 0.01998]])\n",
      " 10%|                                       | 4/40 [00:00<00:06,  5.33it/s]target:  tensor([[0.00000, 0.00000, 0.33678, 0.59380, 0.33707, 0.62007, 0.35067, 0.61177, 0.32698, 0.61041, 0.34149, 0.60164, 0.32843, 0.58897, 0.34183, 0.58096, 0.31799, 0.57756, 0.33227, 0.56912, 0.03267, 0.05095]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.33678, 0.59380, 0.33707, 0.62007, 0.35067, 0.61177, 0.32698, 0.61041, 0.34149, 0.60164, 0.32843, 0.58897, 0.34183, 0.58096, 0.31799, 0.57756, 0.33227, 0.56912, 0.03267, 0.05095]])\n",
      " 12%|                                      | 5/40 [00:00<00:06,  5.40it/s]target:  tensor([[0.00000, 0.00000, 0.03368, 0.21031, 0.00775, 0.18932, 0.01560, 0.18318, 0.01827, 0.23112, 0.02563, 0.22440, 0.03984, 0.19751, 0.04693, 0.19130, 0.04901, 0.23839, 0.05567, 0.23163, 0.04792, 0.05521]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.03368, 0.21031, 0.00775, 0.18932, 0.01560, 0.18318, 0.01827, 0.23112, 0.02563, 0.22440, 0.03984, 0.19751, 0.04693, 0.19130, 0.04901, 0.23839, 0.05567, 0.23163, 0.04792, 0.05521]])\n",
      " 15%|                                     | 6/40 [00:01<00:06,  5.08it/s]target:  tensor([[0.00000, 0.00000, 0.52861, 0.64699, 0.53799, 0.63013, 0.53175, 0.63183, 0.54268, 0.65677, 0.53653, 0.65927, 0.52037, 0.63560, 0.51373, 0.63742, 0.52483, 0.66186, 0.51827, 0.66446, 0.02895, 0.03433]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.52861, 0.64699, 0.53799, 0.63013, 0.53175, 0.63183, 0.54268, 0.65677, 0.53653, 0.65927, 0.52037, 0.63560, 0.51373, 0.63742, 0.52483, 0.66186, 0.51827, 0.66446, 0.02895, 0.03433]])\n",
      " 18%|                                    | 7/40 [00:01<00:06,  5.05it/s]target:  tensor([[0.00000, 0.00000, 0.22680, 0.75571, 0.23993, 0.78422, 0.28210, 0.69398, 0.14864, 0.74979, 0.19982, 0.66425, 0.25887, 0.83697, 0.29480, 0.75128, 0.17786, 0.80588, 0.22096, 0.72402, 0.14616, 0.17273]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.22680, 0.75571, 0.23993, 0.78422, 0.28210, 0.69398, 0.14864, 0.74979, 0.19982, 0.66425, 0.25887, 0.83697, 0.29480, 0.75128, 0.17786, 0.80588, 0.22096, 0.72402, 0.14616, 0.17273]])\n",
      " 20%|                                   | 8/40 [00:01<00:06,  5.15it/s]target:  tensor([[0.00000, 0.00000, 0.59395, 0.04852, 0.60992, 0.04898, 0.59961, 0.02346, 0.58869, 0.07255, 0.57823, 0.04799, 0.60796, 0.04617, 0.59800, 0.02157, 0.58754, 0.06894, 0.57744, 0.04523, 0.03248, 0.05098]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.59395, 0.04852, 0.60992, 0.04898, 0.59961, 0.02346, 0.58869, 0.07255, 0.57823, 0.04799, 0.60796, 0.04617, 0.59800, 0.02157, 0.58754, 0.06894, 0.57744, 0.04523, 0.03248, 0.05098]])\n",
      " 22%|                                  | 9/40 [00:01<00:06,  5.14it/s]target:  tensor([[0.00000, 0.00000, 0.71396, 0.11440, 0.71789, 0.09630, 0.72204, 0.09798, 0.70138, 0.11085, 0.70557, 0.11236, 0.72142, 0.11602, 0.72554, 0.11752, 0.70482, 0.13001, 0.70898, 0.13134, 0.02416, 0.03504]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.71396, 0.11440, 0.71789, 0.09630, 0.72204, 0.09798, 0.70138, 0.11085, 0.70557, 0.11236, 0.72142, 0.11602, 0.72554, 0.11752, 0.70482, 0.13001, 0.70898, 0.13134, 0.02416, 0.03504]])\n",
      " 25%|                                | 10/40 [00:01<00:05,  5.01it/s]target:  tensor([[0.00000, 0.00000, 0.21470, 0.56065, 0.21993, 0.57875, 0.23804, 0.56655, 0.20936, 0.58481, 0.22854, 0.57189, 0.20539, 0.54624, 0.22369, 0.53413, 0.19391, 0.55050, 0.21332, 0.53769, 0.04412, 0.05068]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.21470, 0.56065, 0.21993, 0.57875, 0.23804, 0.56655, 0.20936, 0.58481, 0.22854, 0.57189, 0.20539, 0.54624, 0.22369, 0.53413, 0.19391, 0.55050, 0.21332, 0.53769, 0.04412, 0.05068]])\n",
      " 28%|                               | 11/40 [00:02<00:05,  5.08it/s]target:  tensor([[0.00000, 0.00000, 0.60009, 0.33303, 0.62114, 0.34397, 0.62571, 0.32591, 0.58052, 0.32595, 0.58521, 0.30804, 0.61212, 0.36315, 0.61636, 0.34643, 0.57469, 0.34644, 0.57904, 0.32985, 0.05101, 0.05511]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.60009, 0.33303, 0.62114, 0.34397, 0.62571, 0.32591, 0.58052, 0.32595, 0.58521, 0.30804, 0.61212, 0.36315, 0.61636, 0.34643, 0.57469, 0.34644, 0.57904, 0.32985, 0.05101, 0.05511]])\n",
      " 30%|                              | 12/40 [00:02<00:05,  5.28it/s]target:  tensor([[0.00000, 0.00000, 0.36719, 0.53948, 0.35000, 0.55952, 0.35998, 0.55341, 0.37589, 0.56244, 0.38550, 0.55608, 0.34574, 0.52452, 0.35595, 0.51922, 0.37183, 0.52640, 0.38167, 0.52090, 0.03977, 0.04322]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.36719, 0.53948, 0.35000, 0.55952, 0.35998, 0.55341, 0.37589, 0.56244, 0.38550, 0.55608, 0.34574, 0.52452, 0.35595, 0.51922, 0.37183, 0.52640, 0.38167, 0.52090, 0.03977, 0.04322]])\n",
      " 32%|                             | 13/40 [00:02<00:05,  5.04it/s]target:  tensor([[0.00000, 0.00000, 0.76258, 0.83464, 0.78881, 0.83404, 0.79072, 0.83337, 0.76828, 0.87259, 0.76977, 0.87273, 0.75518, 0.79917, 0.75633, 0.79781, 0.73507, 0.83594, 0.73583, 0.83532, 0.05564, 0.07492]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.76258, 0.83464, 0.78881, 0.83404, 0.79072, 0.83337, 0.76828, 0.87259, 0.76977, 0.87273, 0.75518, 0.79917, 0.75633, 0.79781, 0.73507, 0.83594, 0.73583, 0.83532, 0.05564, 0.07492]])\n",
      " 35%|                            | 14/40 [00:02<00:05,  4.92it/s]target:  tensor([[0.00000, 0.00000, 0.23867, 0.69810, 0.30045, 0.66009, 0.26220, 0.63224, 0.23436, 0.66354, 0.19444, 0.63147, 0.27528, 0.75616, 0.23805, 0.72319, 0.20474, 0.77449, 0.16623, 0.73565, 0.13422, 0.14302]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.23867, 0.69810, 0.30045, 0.66009, 0.26220, 0.63224, 0.23436, 0.66354, 0.19444, 0.63147, 0.27528, 0.75616, 0.23805, 0.72319, 0.20474, 0.77449, 0.16623, 0.73565, 0.13422, 0.14302]])\n",
      " 38%|                          | 15/40 [00:02<00:05,  4.96it/s]target:  tensor([[0.00000, 0.00000, 0.07934, 0.36620, 0.04558, 0.36130, 0.04435, 0.35324, 0.07298, 0.40201, 0.07142, 0.39379, 0.08853, 0.34058, 0.08670, 0.33305, 0.11292, 0.38132, 0.11083, 0.37361, 0.06856, 0.06896]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.07934, 0.36620, 0.04558, 0.36130, 0.04435, 0.35324, 0.07298, 0.40201, 0.07142, 0.39379, 0.08853, 0.34058, 0.08670, 0.33305, 0.11292, 0.38132, 0.11083, 0.37361, 0.06856, 0.06896]])\n",
      " 40%|                         | 16/40 [00:03<00:04,  5.03it/s]target:  tensor([[0.00000, 0.00000, 0.28571, 0.62805, 0.31372, 0.69332, 0.33268, 0.66475, 0.26922, 0.65298, 0.29161, 0.62153, 0.27305, 0.64380, 0.29139, 0.61809, 0.22712, 0.60060, 0.24857, 0.57264, 0.10557, 0.12067]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.28571, 0.62805, 0.31372, 0.69332, 0.33268, 0.66475, 0.26922, 0.65298, 0.29161, 0.62153, 0.27305, 0.64380, 0.29139, 0.61809, 0.22712, 0.60060, 0.24857, 0.57264, 0.10557, 0.12067]])\n",
      " 42%|                        | 17/40 [00:03<00:04,  4.81it/s]target:  tensor([[0.00000, 0.00000, 0.72492, 0.54889, 0.71709, 0.56332, 0.71989, 0.55463, 0.71210, 0.54435, 0.71475, 0.53571, 0.73443, 0.56498, 0.73744, 0.55630, 0.72890, 0.54594, 0.73176, 0.53730, 0.02535, 0.02927]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.72492, 0.54889, 0.71709, 0.56332, 0.71989, 0.55463, 0.71210, 0.54435, 0.71475, 0.53571, 0.73443, 0.56498, 0.73744, 0.55630, 0.72890, 0.54594, 0.73176, 0.53730, 0.02535, 0.02927]])\n",
      " 45%|                       | 18/40 [00:03<00:04,  4.88it/s]target:  tensor([[0.00000, 0.00000, 0.52620, 0.15324, 0.53156, 0.15024, 0.52949, 0.14179, 0.52192, 0.15470, 0.51987, 0.14625, 0.53195, 0.15835, 0.52993, 0.15006, 0.52250, 0.16270, 0.52050, 0.15442, 0.01207, 0.02091]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.52620, 0.15324, 0.53156, 0.15024, 0.52949, 0.14179, 0.52192, 0.15470, 0.51987, 0.14625, 0.53195, 0.15835, 0.52993, 0.15006, 0.52250, 0.16270, 0.52050, 0.15442, 0.01207, 0.02091]])\n",
      " 48%|                      | 19/40 [00:03<00:04,  5.10it/s]target:  tensor([[0.00000, 0.00000, 0.87506, 0.21395, 0.86890, 0.18262, 0.91538, 0.16248, 0.83903, 0.19152, 0.88079, 0.17337, 0.87428, 0.25454, 0.91950, 0.23601, 0.84470, 0.25679, 0.88544, 0.23997, 0.08047, 0.09431]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.87506, 0.21395, 0.86890, 0.18262, 0.91538, 0.16248, 0.83903, 0.19152, 0.88079, 0.17337, 0.87428, 0.25454, 0.91950, 0.23601, 0.84470, 0.25679, 0.88544, 0.23997, 0.08047, 0.09431]])\n",
      " 50%|                     | 20/40 [00:03<00:03,  5.09it/s]target:  tensor([[0.00000, 0.00000, 0.76368, 0.25611, 0.77822, 0.24191, 0.76247, 0.24070, 0.76770, 0.24284, 0.75187, 0.24158, 0.77348, 0.27024, 0.75810, 0.26841, 0.76304, 0.27177, 0.74758, 0.26987, 0.03063, 0.03107]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.76368, 0.25611, 0.77822, 0.24191, 0.76247, 0.24070, 0.76770, 0.24284, 0.75187, 0.24158, 0.77348, 0.27024, 0.75810, 0.26841, 0.76304, 0.27177, 0.74758, 0.26987, 0.03063, 0.03107]])\n",
      " 52%|                    | 21/40 [00:04<00:03,  5.09it/s]target:  tensor([[0.00000, 0.00000, 0.25791, 0.83875, 0.26752, 0.85036, 0.24518, 0.86106, 0.27102, 0.85298, 0.24964, 0.86328, 0.26366, 0.81531, 0.24172, 0.82524, 0.26724, 0.81925, 0.24623, 0.82885, 0.02930, 0.04797]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.25791, 0.83875, 0.26752, 0.85036, 0.24518, 0.86106, 0.27102, 0.85298, 0.24964, 0.86328, 0.26366, 0.81531, 0.24172, 0.82524, 0.26724, 0.81925, 0.24623, 0.82885, 0.02930, 0.04797]])\n",
      " 55%|                   | 22/40 [00:04<00:03,  5.12it/s]target:  tensor([[0.00000, 0.00000, 0.04136, 0.46271, 0.00938, 0.47198, 0.00955, 0.45627, 0.03747, 0.47525, 0.03760, 0.46052, 0.04550, 0.47140, 0.04564, 0.45578, 0.07118, 0.47469, 0.07129, 0.46004, 0.06190, 0.01947]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.04136, 0.46271, 0.00938, 0.47198, 0.00955, 0.45627, 0.03747, 0.47525, 0.03760, 0.46052, 0.04550, 0.47140, 0.04564, 0.45578, 0.07118, 0.47469, 0.07129, 0.46004, 0.06190, 0.01947]])\n",
      " 57%|                  | 23/40 [00:04<00:03,  5.09it/s]target:  tensor([[0.00000, 0.00000, 0.47606, 0.44015, 0.47080, 0.46363, 0.46233, 0.47170, 0.49969, 0.45178, 0.49032, 0.46008, 0.45839, 0.41999, 0.45047, 0.42982, 0.48755, 0.40954, 0.47870, 0.41949, 0.04922, 0.06216]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.47606, 0.44015, 0.47080, 0.46363, 0.46233, 0.47170, 0.49969, 0.45178, 0.49032, 0.46008, 0.45839, 0.41999, 0.45047, 0.42982, 0.48755, 0.40954, 0.47870, 0.41949, 0.04922, 0.06216]])\n",
      " 60%|                 | 24/40 [00:04<00:02,  5.40it/s]target:  tensor([[0.00000, 0.00000, 0.51454, 0.30261, 0.51752, 0.30282, 0.51052, 0.28922, 0.53525, 0.29891, 0.52816, 0.28460, 0.50363, 0.32382, 0.49635, 0.31032, 0.52094, 0.32088, 0.51355, 0.30667, 0.03890, 0.03922]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.51454, 0.30261, 0.51752, 0.30282, 0.51052, 0.28922, 0.53525, 0.29891, 0.52816, 0.28460, 0.50363, 0.32382, 0.49635, 0.31032, 0.52094, 0.32088, 0.51355, 0.30667, 0.03890, 0.03922]])\n",
      " 62%|                | 25/40 [00:04<00:02,  5.02it/s]target:  tensor([[0.00000, 0.00000, 0.11088, 0.56701, 0.05097, 0.58819, 0.06520, 0.51686, 0.08153, 0.59917, 0.09341, 0.53532, 0.13174, 0.59389, 0.14312, 0.52518, 0.15250, 0.60387, 0.16215, 0.54213, 0.11118, 0.08702]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.11088, 0.56701, 0.05097, 0.58819, 0.06520, 0.51686, 0.08153, 0.59917, 0.09341, 0.53532, 0.13174, 0.59389, 0.14312, 0.52518, 0.15250, 0.60387, 0.16215, 0.54213, 0.11118, 0.08702]])\n",
      " 65%|               | 26/40 [00:05<00:02,  5.10it/s]target:  tensor([[ 0.00000,  0.00000,  0.00537,  0.16343,  0.02477,  0.12532,  0.03320,  0.12683, -0.04769,  0.12130, -0.03424,  0.12315,  0.04492,  0.20236,  0.05182,  0.19935, -0.02309,  0.20235, -0.01174,  0.19920,  0.09951,  0.08106]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[ 0.00000,  0.00537,  0.16343,  0.02477,  0.12532,  0.03320,  0.12683, -0.04769,  0.12130, -0.03424,  0.12315,  0.04492,  0.20236,  0.05182,  0.19935, -0.02309,  0.20235, -0.01174,  0.19920,  0.09951,  0.08106]])\n",
      " 68%|              | 27/40 [00:05<00:02,  4.76it/s]target:  tensor([[0.00000, 0.00000, 0.08041, 0.43545, 0.08378, 0.44862, 0.07019, 0.44355, 0.08089, 0.44534, 0.06753, 0.44032, 0.09033, 0.42889, 0.07690, 0.42350, 0.08732, 0.42601, 0.07411, 0.42069, 0.02280, 0.02793]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.08041, 0.43545, 0.08378, 0.44862, 0.07019, 0.44355, 0.08089, 0.44534, 0.06753, 0.44032, 0.09033, 0.42889, 0.07690, 0.42350, 0.08732, 0.42601, 0.07411, 0.42069, 0.02280, 0.02793]])\n",
      " 70%|             | 28/40 [00:05<00:02,  4.66it/s]target:  tensor([[0.00000, 0.00000, 0.47148, 0.45415, 0.45824, 0.43175, 0.46899, 0.42555, 0.45523, 0.44644, 0.46533, 0.44052, 0.47379, 0.47175, 0.48454, 0.46523, 0.46978, 0.48408, 0.47989, 0.47789, 0.02932, 0.05853]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.47148, 0.45415, 0.45824, 0.43175, 0.46899, 0.42555, 0.45523, 0.44644, 0.46533, 0.44052, 0.47379, 0.47175, 0.48454, 0.46523, 0.46978, 0.48408, 0.47989, 0.47789, 0.02932, 0.05853]])\n",
      " 72%|           | 29/40 [00:05<00:02,  5.04it/s]target:  tensor([[0.00000, 0.00000, 0.96440, 0.75960, 0.97596, 0.76996, 0.97420, 0.77048, 0.95784, 0.76839, 0.95590, 0.76888, 0.97239, 0.74995, 0.97060, 0.75022, 0.95448, 0.74877, 0.95252, 0.74903, 0.02345, 0.02171]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.96440, 0.75960, 0.97596, 0.76996, 0.97420, 0.77048, 0.95784, 0.76839, 0.95590, 0.76888, 0.97239, 0.74995, 0.97060, 0.75022, 0.95448, 0.74877, 0.95252, 0.74903, 0.02345, 0.02171]])\n",
      " 75%|          | 30/40 [00:05<00:01,  5.38it/s]target:  tensor([[0.00000, 0.00000, 0.11201, 0.61413, 0.09442, 0.61019, 0.10497, 0.60865, 0.10368, 0.63308, 0.11400, 0.63092, 0.11282, 0.59622, 0.12285, 0.59507, 0.12209, 0.61903, 0.13189, 0.61725, 0.03747, 0.03801]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.11201, 0.61413, 0.09442, 0.61019, 0.10497, 0.60865, 0.10368, 0.63308, 0.11400, 0.63092, 0.11282, 0.59622, 0.12285, 0.59507, 0.12209, 0.61903, 0.13189, 0.61725, 0.03747, 0.03801]])\n",
      " 78%|         | 31/40 [00:06<00:01,  5.66it/s]target:  tensor([[0.00000, 0.00000, 0.22960, 0.64040, 0.20237, 0.67660, 0.21833, 0.65861, 0.24970, 0.67490, 0.26277, 0.65711, 0.19705, 0.61679, 0.21360, 0.60213, 0.24524, 0.61538, 0.25879, 0.60089, 0.06573, 0.07571]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.22960, 0.64040, 0.20237, 0.67660, 0.21833, 0.65861, 0.24970, 0.67490, 0.26277, 0.65711, 0.19705, 0.61679, 0.21360, 0.60213, 0.24524, 0.61538, 0.25879, 0.60089, 0.06573, 0.07571]])\n",
      " 80%|        | 32/40 [00:06<00:01,  5.54it/s]target:  tensor([[0.00000, 0.00000, 0.01364, 0.13938, 0.02304, 0.15082, 0.02569, 0.14249, 0.00231, 0.13702, 0.00505, 0.12850, 0.02115, 0.15228, 0.02377, 0.14408, 0.00072, 0.13875, 0.00341, 0.13036, 0.02498, 0.02378]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.01364, 0.13938, 0.02304, 0.15082, 0.02569, 0.14249, 0.00231, 0.13702, 0.00505, 0.12850, 0.02115, 0.15228, 0.02377, 0.14408, 0.00072, 0.13875, 0.00341, 0.13036, 0.02498, 0.02378]])\n",
      " 82%|       | 33/40 [00:06<00:01,  5.47it/s]target:  tensor([[0.00000, 0.00000, 0.22541, 0.65650, 0.19967, 0.68776, 0.20352, 0.67769, 0.20069, 0.63972, 0.20438, 0.62944, 0.24467, 0.68710, 0.24898, 0.67753, 0.24365, 0.64138, 0.24775, 0.63161, 0.04931, 0.05832]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.22541, 0.65650, 0.19967, 0.68776, 0.20352, 0.67769, 0.20069, 0.63972, 0.20438, 0.62944, 0.24467, 0.68710, 0.24898, 0.67753, 0.24365, 0.64138, 0.24775, 0.63161, 0.04931, 0.05832]])\n",
      " 85%|      | 34/40 [00:06<00:01,  5.39it/s]target:  tensor([[0.00000, 0.00000, 0.40237, 0.93000, 0.45663, 0.86364, 0.47738, 0.90205, 0.38684, 0.99057, 0.40254, 1.04287, 0.39762, 0.81118, 0.41468, 0.84804, 0.31835, 0.94557, 0.32870, 0.99825, 0.15903, 0.23169]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.40237, 0.93000, 0.45663, 0.86364, 0.47738, 0.90205, 0.38684, 0.99057, 0.40254, 1.04287, 0.39762, 0.81118, 0.41468, 0.84804, 0.31835, 0.94557, 0.32870, 0.99825, 0.15903, 0.23169]])\n",
      " 88%|     | 35/40 [00:06<00:01,  4.99it/s]target:  tensor([[0.00000, 0.00000, 0.69739, 0.48835, 0.70485, 0.48159, 0.69768, 0.47667, 0.70124, 0.49437, 0.69403, 0.48941, 0.69887, 0.48631, 0.69187, 0.48149, 0.69531, 0.49884, 0.68825, 0.49398, 0.01660, 0.02217]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.69739, 0.48835, 0.70485, 0.48159, 0.69768, 0.47667, 0.70124, 0.49437, 0.69403, 0.48941, 0.69887, 0.48631, 0.69187, 0.48149, 0.69531, 0.49884, 0.68825, 0.49398, 0.01660, 0.02217]])\n",
      " 90%|    | 36/40 [00:07<00:00,  5.05it/s]target:  tensor([[0.00000, 0.00000, 0.43260, 0.29080, 0.42181, 0.26822, 0.43033, 0.26728, 0.43228, 0.28074, 0.44044, 0.27990, 0.42092, 0.30272, 0.42945, 0.30196, 0.43145, 0.31360, 0.43962, 0.31293, 0.01952, 0.04632]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.43260, 0.29080, 0.42181, 0.26822, 0.43033, 0.26728, 0.43228, 0.28074, 0.44044, 0.27990, 0.42092, 0.30272, 0.42945, 0.30196, 0.43145, 0.31360, 0.43962, 0.31293, 0.01952, 0.04632]])\n",
      " 92%|   | 37/40 [00:07<00:00,  4.99it/s]target:  tensor([[ 0.00000e+00,  0.00000e+00,  2.30043e-02,  2.94618e-01,  5.30616e-02,  3.25629e-01,  4.77124e-02,  3.24951e-01,  4.63564e-02,  2.68396e-01,  4.06911e-02,  2.65831e-01,  7.56208e-03,  3.23490e-01,  5.15440e-04,  3.22718e-01, -1.90848e-04,  2.64285e-01, -7.63178e-03,  2.61492e-01,  6.06934e-02,  6.41374e-02]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[ 0.00000e+00,  2.30043e-02,  2.94618e-01,  5.30616e-02,  3.25629e-01,  4.77124e-02,  3.24951e-01,  4.63564e-02,  2.68396e-01,  4.06911e-02,  2.65831e-01,  7.56208e-03,  3.23490e-01,  5.15440e-04,  3.22718e-01, -1.90848e-04,  2.64285e-01, -7.63178e-03,  2.61492e-01,  6.06934e-02,  6.41374e-02]])\n",
      " 95%|  | 38/40 [00:07<00:00,  4.85it/s]target:  tensor([[ 0.00000,  0.00000,  0.08215,  0.32781,  0.02812,  0.40429, -0.00337,  0.27929,  0.02282,  0.36299, -0.00480,  0.25178,  0.16055,  0.38603,  0.14350,  0.27627,  0.14098,  0.35131,  0.12463,  0.25235,  0.16535,  0.15251]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[ 0.00000,  0.08215,  0.32781,  0.02812,  0.40429, -0.00337,  0.27929,  0.02282,  0.36299, -0.00480,  0.25178,  0.16055,  0.38603,  0.14350,  0.27627,  0.14098,  0.35131,  0.12463,  0.25235,  0.16535,  0.15251]])\n",
      " 98%| | 39/40 [00:07<00:00,  4.46it/s]target:  tensor([[0.00000, 0.00000, 0.16666, 0.36641, 0.20232, 0.37824, 0.20049, 0.36362, 0.16774, 0.33331, 0.16750, 0.32008, 0.16579, 0.41114, 0.16566, 0.39482, 0.12859, 0.36767, 0.13031, 0.35258, 0.07373, 0.09106]])\n",
      "/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "tensor([[0.00000, 0.16666, 0.36641, 0.20232, 0.37824, 0.20049, 0.36362, 0.16774, 0.33331, 0.16750, 0.32008, 0.16579, 0.41114, 0.16566, 0.39482, 0.12859, 0.36767, 0.13031, 0.35258, 0.07373, 0.09106]])\n",
      "100%|| 40/40 [00:07<00:00,  5.03it/s]\n",
      "-----------------------------------\n",
      "  tensor to cuda : 0.000754\n",
      "         predict : 0.081645\n",
      "    compute loss : 0.000010\n",
      "get_region_boxes : 0.000270\n",
      "            eval : 0.129094\n",
      "             pnp : 0.000179\n",
      "           total : 0.082680\n",
      "-----------------------------------\n",
      "   Mean corner error is 120.712662\n",
      "   Acc using 5 px 2D Projection = 22.50%\n",
      "   Acc using 0.020000000000000004 vx 3D Transformation = 2.50%\n",
      "   Acc using 5 cm 5 degree metric = 2.50%\n",
      "   Translation error: 11.503188, angle error: 96.368773\n"
     ]
    }
   ],
   "source": [
    "!python test.py --batch 1 --weights runs/train/exp19/weights/best.pt --data configs/drone_data/drone.yaml --save-json --single-cls --save-json --test-plotting --save-conf --save-hybrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights=['runs/train/exp19/weights/best.pt'], data='configs/drone_data/drone_real.yaml', batch_size=1, img_size=640, conf_thres=0.01, num_keypoints=9, task='val', device='cpu', single_cls=True, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project='runs/test', name='exp', exist_ok=False, symetric=False, test_plotting=True)\n",
      "YOLOv5 14469dd torch 1.9.1+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 226 layers, 8894676 parameters, 0 gradients, 19.4 GFLOPS\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../data/real_dataset/drone/test.cache' for images, labels, masks.\u001b[0m\n",
      "  0%|                                                  | 0/2082 [00:00<?, ?it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  0%|                                          | 1/2082 [00:00<17:21,  2.00it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  0%|                                          | 2/2082 [00:00<16:36,  2.09it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  0%|                                          | 3/2082 [00:01<16:19,  2.12it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  0%|                                          | 4/2082 [00:01<16:11,  2.14it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  0%|                                          | 5/2082 [00:02<16:14,  2.13it/s]/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  0%|                                          | 5/2082 [00:02<17:44,  1.95it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/test.py\", line 393, in <module>\n",
      "    test(opt.data,\n",
      "  File \"/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/test.py\", line 299, in test\n",
      "    fig.savefig(file_path, dpi = 96, bbox_inches='tight', pad_inches=0)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/figure.py\", line 3390, in savefig\n",
      "    self.canvas.print_figure(fname, **kwargs)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/backend_bases.py\", line 2164, in print_figure\n",
      "    self.figure.draw(renderer)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/artist.py\", line 95, in draw_wrapper\n",
      "    result = draw(artist, renderer, *args, **kwargs)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/artist.py\", line 72, in draw_wrapper\n",
      "    return draw(artist, renderer)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/figure.py\", line 3154, in draw\n",
      "    mimage._draw_list_compositing_images(\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/image.py\", line 132, in _draw_list_compositing_images\n",
      "    a.draw(renderer)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/artist.py\", line 72, in draw_wrapper\n",
      "    return draw(artist, renderer)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/axes/_base.py\", line 3070, in draw\n",
      "    mimage._draw_list_compositing_images(\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/image.py\", line 132, in _draw_list_compositing_images\n",
      "    a.draw(renderer)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/artist.py\", line 72, in draw_wrapper\n",
      "    return draw(artist, renderer)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/image.py\", line 649, in draw\n",
      "    im, l, b, trans = self.make_image(\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/image.py\", line 939, in make_image\n",
      "    return self._make_image(self._A, bbox, transformed_bbox, clip,\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/image.py\", line 559, in _make_image\n",
      "    A = _rgb_to_rgba(A)\n",
      "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/matplotlib/image.py\", line 223, in _rgb_to_rgba\n",
      "    rgba[:, :, :3] = A\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python test.py --batch 1 --weights runs/train/exp19/weights/best.pt --data configs/drone_data/drone_real.yaml --save-json --single-cls --save-json --test-plotting save-conf save-hybrid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
