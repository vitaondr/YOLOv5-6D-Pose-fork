{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# 1. Setup\n",
        "\n",
        "Clone repo, install dependencies and check PyTorch and GPU.\n",
        "\n",
        "Alternatively, you can build the docker image or download our pre-built [image](https://hub.docker.com/repository/docker/sudochris/yolo6dpose/general) to reproduce the experiments\n",
        "\n",
        "The code has been tested on Python 3.9.1 on both windows 11 and Ubunutu 20.04"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "ae8805a9-ce15-4e1c-f6b4-baa1c1033f56"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cviviers/YOLOv5-6D-Pose.git  # clone\n",
        "!cd YOLOv5-6D-Pose\n",
        "!pip install -r requirements.txt  # install\n",
        "\n",
        "# (Optional) install dependecies for calculating the 6D pose of symmetric objects\n",
        "# This requires python3.XX-dev (your version of python) in addition to a C++ compiler eg. sudo apt-get install python3.9-dev\n",
        "!cd ./utils/\n",
        "!python ./setup.py build_ext --inplace  # you will need to install cython # https://stackoverflow.com/questions/7508803/how-do-i-import-function-from-pyx-file-in-python\n",
        "\n",
        "import torch\n",
        "from IPython.display import clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/utils/setup.py\", line 6, in <module>\n",
            "    setup(ext_modules=cythonize([package]))\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/Cython/Build/Dependencies.py\", line 1010, in cythonize\n",
            "    module_list, module_metadata = create_extension_list(\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/Cython/Build/Dependencies.py\", line 845, in create_extension_list\n",
            "    for file in nonempty(sorted(extended_iglob(filepattern)), \"'%s' doesn't match any files\" % filepattern):\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/Cython/Build/Dependencies.py\", line 117, in nonempty\n",
            "    raise ValueError(error_msg)\n",
            "ValueError: 'compute_overlap.pyx' doesn't match any files\n"
          ]
        }
      ],
      "source": [
        "# (Optional) install dependecies for calculating the 6D pose of symmetric objects\n",
        "# This requires python3.XX-dev (your version of python) in addition to a C++ compiler eg. sudo apt-get install python3.9-dev\n",
        "!cd ./utils/\n",
        "!python ./setup.py build_ext --inplace  # you will need to install cython # https://stackoverflow.com/questions/7508803/how-do-i-import-function-from-pyx-file-in-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.9.1+cu102 CPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from IPython.display import clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example project layout\n",
        "\n",
        "```\n",
        "├── data\n",
        "│   ├── LINEMOD\n",
        "│   ├── VOCdevkit\n",
        "│   ├── Weights    \n",
        "├── yolov5_6d_pose\n",
        "│   ├── tutorial.ipynb\n",
        "│   ├── train.py\n",
        "│   ├── detect.py\n",
        "│   ├── etc...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 2. Inference\n",
        "\n",
        "1. Download the pretrained weights from [google_drive](https://drive.google.com/drive/folders/11BW41xO3R1UBnc2Dx1xA3CPbYPGTrfHQ?usp=drive_link)\n",
        "2. Extract them to a preferred location\n",
        "3. `detect.py` runs inference on a variety of sources. In this example we included an image from the LINEMOD dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "tar: Error opening archive: Failed to open 'weights.tar'\n"
          ]
        }
      ],
      "source": [
        "# Extract YOLOv5-6D pretrained weights, this includes the weights for the intialization and the fine-tuned models for each of the objects\n",
        "!tar xf linemod_weights.tar -C ../weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "zR9ZbuQCH7FX",
        "outputId": "c9a308f7-2216-4805-8003-eca8dd0dc30d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(weights=['../data/LINEMOD/cat/weights/best.pt'], source='results/images/example_cat.jpg', static_camera='configs/linemod/linemod_camera.json', mesh_data='../data/LINEMOD/cat/cat.ply', img_size=640, conf_thres=0.25, device='', view_img=False, name='exp', exist_ok=False, project='runs/detect')\n",
            "YOLOv5 6b982c6 torch 1.9.1+cu102 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 391 layers, 97574820 parameters, 0 gradients, 225.4 GFLOPS\n",
            "image 1/1 /home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/results/images/example_cat.jpg: tensor([[364.15668, 161.39651, 332.66818, 173.51559, 323.54919, 125.69576, 398.54477, 161.72449, 394.68384, 111.85746, 335.92682, 197.77258, 326.57761, 149.67729, 404.42773, 185.40781, 401.13223, 135.72612,   0.85526,   0.00000]])\n",
            "480x640 1 cat, Done. (0.571s)\n",
            "Results saved to runs/detect/exp2\n",
            "1 labels saved to runs/detect/exp2/labels\n",
            "Done. (1.076s)\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights ../data/LINEMOD/cat/weights/best.pt --img 640 --conf 0.25 --source results/images/example_cat.jpg --static-camera configs/linemod/linemod_camera.json --mesh-data ../data/LINEMOD/cat/cat.ply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "# 3. Test\n",
        "Test a model on [LINEMOD](https://arxiv.org/abs/1808.08319)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download LINEMOD the original dataset\n",
        "!wget -O LINEMOD.tar --no-check-certificate \"https://onedrive.live.com/download?cid=05750EBEE1537631&resid=5750EBEE1537631%21135&authkey=AJRHFmZbcjXxTmI\"\n",
        "!tar xf LINEMOD.tar -C ../data\n",
        "\n",
        "# Be sure to convert the data to our format focusing on the camera intrinsics\n",
        "!python data_curation/convert_linemod.py --path_to_linemod ../data/LINEMOD\n",
        "\n",
        "# Ulternatively, you can download the converted dataset from here: https://drive.google.com/drive/folders/13pkdF4KpqXWXbEIkiAcHoO-YUr_CWN-L?usp=drive_link\n",
        "!tar xf LINEMOD_updated.tar -C ../data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test LINEMOD cat \n",
        "!python test.py --batch 1 --weights ../data/weights/linemod/cat/best.pt --data configs/linemod/cat.yaml --rect --cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(weights=['../data/LINEMOD/cat/weights/best.pt'], data='configs/linemod/cat.yaml', batch_size=1, img_size=640, conf_thres=0.01, num_keypoints=9, task='val', device='', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='runs/test', name='exp', exist_ok=False, symetric=False, test_plotting=False)\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 3016, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 2813, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/test.py\", line 390, in <module>\n",
            "    check_requirements()\n",
            "  File \"/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/utils/general.py\", line 78, in handler\n",
            "    func(*args, **kwargs)\n",
            "  File \"/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/utils/general.py\", line 248, in check_requirements\n",
            "    pkg.require(r)\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 886, in require\n",
            "    needed = self.resolve(parse_requirements(requirements))\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 780, in resolve\n",
            "    new_requirements = dist.requires(req.extras)[::-1]\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 2734, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 3018, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 3040, in _compute_dependencies\n",
            "    dm[s_extra] = list(frozenset(reqs_for_extra(extra)) - common)\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 3032, in reqs_for_extra\n",
            "    if not req.marker or req.marker.evaluate({'extra': extra}):\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/pkg_resources/_vendor/packaging/markers.py\", line 328, in evaluate\n",
            "    return _evaluate_markers(self._markers, current_environment)\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/pkg_resources/_vendor/packaging/markers.py\", line 250, in _evaluate_markers\n",
            "    groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/pkg_resources/_vendor/packaging/markers.py\", line 197, in _eval_op\n",
            "    spec = Specifier(\"\".join([op.serialize(), rhs]))\n",
            "  File \"/home/ondra/anaconda3/envs/yolov5/lib/python3.9/site-packages/pkg_resources/_vendor/packaging/specifiers.py\", line 110, in __init__\n",
            "    match = self._regex.search(spec)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python test.py --batch 1 --weights ../data/LINEMOD/cat/weights/best.pt --data configs/linemod/cat.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUOiNLtMP5aG"
      },
      "source": [
        "# 4. Train\n",
        "\n",
        "Download [LINEMOD](https://bop.felk.cvut.cz/) and [VOCdevkit](http://host.robots.ox.ac.uk/pascal/VOC/index.html), start tensorboard/wnadb and train YOLOv5-6D (note actual training is typically much longer, around **300-6000 epochs**, depending on your dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "e6459e0bcee449b090fc9807672725bc",
            "c341e1d3bf3b40d1821ce392eb966c68",
            "660afee173694231a6dce3cd94df6cae",
            "261218485cef48df961519dde5edfcbe",
            "32736d503c06497abfae8c0421918255",
            "e257738711f54d5280c8393d9d3dce1c",
            "beb7a6fe34b840899bb79c062681696f",
            "e639132395d64d70b99d8b72c32f8fbb"
          ]
        },
        "id": "Knxi2ncxWffW",
        "outputId": "e8b7d5b3-a71e-4446-eec2-ad13419cf700"
      },
      "outputs": [],
      "source": [
        "# Download VOC, This can also be any other dataset you want to use for background images. We also use VOC for occlusion augmentation\n",
        "!wget https://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar  # Alternatively - http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar or https://drive.google.com/drive/folders/13pkdF4KpqXWXbEIkiAcHoO-YUr_CWN-L?usp=drive_link\n",
        "!tar xf VOCtrainval_11-May-2012.tar -C ../data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bOy5KI2ncnWd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-abf844ec430cf859\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-abf844ec430cf859\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Tensorboard (optional)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "%reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2fLAV42oNb7M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvitaondr\u001b[0m (\u001b[33mvitaondr-czech-technical-university-in-prague\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "# Weights & Biases (optional)\n",
        "%pip install -q wandb  \n",
        "\n",
        "!wandb login  # use 'wandb disabled' or 'wandb enabled' to disable or enable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training configuration\n",
        "```bash\n",
        "python train.py --batch \"batch size\" --epochs \"number of epochs\" --cfg \"model configuration\" --hyp \"hyper parameter configuration\" --weights \"starting weights\" --data \"data configuration\" --rect --cache --optimizer Adam \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NcFxRcFdJ_O",
        "outputId": "38e51b29-2df4-4f00-cde8-5f6e4a34da9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLOv5 6b982c6 torch 1.9.1+cu102 CPU\n",
            "\n",
            "Namespace(weights='../data/weights/yolov5x.pt', cfg='models/yolov5x_6dpose_bifpn.yaml', data='configs/linemod/benchvise.yaml', hyp='configs/hyp.single.yaml', epochs=5000, batch_size=32, img_size=[640, 640], rect=True, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=True, image_weights=False, device='', single_cls=False, optimizer='Adam', sync_bn=False, local_rank=-1, log_imgs=8, log_artifacts=False, workers=8, project='runs/train', entity=None, name='exp', exist_ok=False, quad=False, linear_lr=False, standard_lr=False, symetric=False, world_size=1, global_rank=-1, save_dir='runs/train/exp13', total_batch_size=32)\n",
            "Start Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0001, lrf=0.01, lr_factor=0.5, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=1.5, obj=0.1, obj_pw=1.0, cls=0.5, cls_pw=1.0, anchor_t=4.0, hsv_h=0.03, hsv_s=0.7, hsv_v=0.4, degrees=30.0, translate=0.2, scale=0.2, shear=0, perspective=0, blur=0, blur_kernel=5, flipud=False, fliplr=False, background=0.5, multi_scale=False, occlude=0.3, theta=0.2, alpha=2, pretrain_epochs=40\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      8800  models.common.Conv                      [3, 80, 6, 2, 2]              \n",
            "  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
            "  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \n",
            "  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n",
            "  4                -1  8   2259200  models.common.C3                        [320, 320, 8]                 \n",
            "  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n",
            "  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \n",
            "  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n",
            "  8                -1  4  19676160  models.common.C3                        [1280, 1280, 4]               \n",
            "  9                -1  1   4099840  models.common.SPPF                      [1280, 1280, 5]               \n",
            " 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \n",
            " 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \n",
            " 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
            " 19       [-1, 14, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \n",
            " 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \n",
            " 24      [17, 20, 23]  1  11056820  models.yolo.Pose                        [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [320, 640, 1280]]\n",
            "Model Summary: 547 layers, 97643860 parameters, 97643860 gradients, 226.6 GFLOPS\n",
            "\n",
            "Transferred 736/944 items from ../data/weights/yolov5x.pt\n",
            "Transferred 736/944 items from ../data/weights/yolov5x.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "Optimizer groups: 159 .bias, 159 conv.weight, 156 other\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvitaondr\u001b[0m (\u001b[33mvitaondr-czech-technical-university-in-prague\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ondra/BC/yolov5/YOLOv5-6D-Pose-fork/wandb/run-20250227_163102-wvkej6o5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexp13\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/vitaondr-czech-technical-university-in-prague/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/vitaondr-czech-technical-university-in-prague/YOLOv5/runs/wvkej6o5\u001b[0m\n",
            "Creating occluders from VOC: ../data/VOCdevkit/VOC2012\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../data/LINEMOD/benchvise/train.cache' for images, labels, mask\u001b[0m\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB): 100%|████████| 183/183 [00:00<00:00, 1826.59it/s]\u001b[0m\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../data/LINEMOD/benchvise/test.cache' for images, labels, masks..\u001b[0m\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (1.0GB): 100%|████████| 1032/1032 [00:00<00:00, 1054.79it/s]\u001b[0m\n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 0.00, Best Possible Recall (BPR) = 0.0000. Attempting to improve anchors, please wait...\n",
            "<utils.datasets.LoadImagesAndLabelsPose object at 0x7f9ac0484fa0>\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 183 points...\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 9.00 anchors past thr\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.731/0.925-mean/best, past_thr=0.731-mean: 112,183,  143,149,  151,180,  176,170,  172,202,  149,256,  214,213,  208,266,  256,252\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.9264: 100%|█| 1\u001b[0m\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 9.00 anchors past thr\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.734/0.926-mean/best, past_thr=0.734-mean: 113,180,  146,152,  151,182,  175,169,  169,197,  151,257,  212,213,  208,261,  255,252\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
            "\n",
            "Image sizes 640 train, 640 test\n",
            "Using 8 dataloader workers\n",
            "Logging results to runs/train/exp13\n",
            "Starting training for 5000 epochs...\n",
            "\n",
            "     Epoch   gpu_mem     l_obj     l_box     l_cls n_targets  img_size\n",
            "  0%|                                                     | 0/6 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "# By default, the training script will download the COCO pretrained YOLOv5-v7 model. If this is no longer supported, you can download the weights from the YOLOv5-6D google drive (https://drive.google.com/drive/folders/11BW41xO3R1UBnc2Dx1xA3CPbYPGTrfHQ?usp=drive_link) and place them in the weights folder\n",
        "!python train.py --batch 32 --epochs 5000 --cfg models/yolov5x_6dpose_bifpn.yaml --hyp configs/hyp.single.yaml --weights ../data/weights/yolov5x.pt --data configs/linemod/benchvise.yaml --rect --cache --optimizer Adam "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15glLzbQx5u0"
      },
      "source": [
        "# 5. Visualize\n",
        "## Weights & Biases Logging\n",
        "\n",
        "[Weights & Biases](https://www.wandb.com/) (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W&B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n",
        "\n",
        "During training you will see live updates at [https://wandb.ai/home](https://wandb.ai/home)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPvRbS5Swl6"
      },
      "source": [
        "## Local Logging\n",
        "\n",
        "All results are logged by default to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc. View train and test pngs to see labels, predictions and augmentation effects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KN5ghjE6ZWh"
      },
      "source": [
        "Training losses and performance metrics are also logged to [Tensorboard](https://www.tensorflow.org/tensorboard) and a custom `results.txt` logfile which is plotted as `results.png` (below) after training completes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zelyeqbyt3GD"
      },
      "source": [
        "# Environments\n",
        "\n",
        "YOLOv5-6D may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n",
        "\n",
        "- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
        "- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "YOLOv5 Tutorial",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "yolov5",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a1246a73077468ab80e979cc0576cd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f8e9b8ebded4175b2eaa9f75c3ceb00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d327cde5a85a4a51bb8b1b3e9cf06c97",
              "IPY_MODEL_d5ef1cb2cbed4b87b3c5d292ff2b0da6"
            ],
            "layout": "IPY_MODEL_0a1246a73077468ab80e979cc0576cd2"
          }
        },
        "261218485cef48df961519dde5edfcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e639132395d64d70b99d8b72c32f8fbb",
            "placeholder": "​",
            "style": "IPY_MODEL_beb7a6fe34b840899bb79c062681696f",
            "value": " 21.1M/21.1M [00:00&lt;00:00, 33.5MB/s]"
          }
        },
        "2919396dbd4b4c8e821d12bd28665d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32736d503c06497abfae8c0421918255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "3d5136c19e7645ca9bc8f51ceffb2be1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "660afee173694231a6dce3cd94df6cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e257738711f54d5280c8393d9d3dce1c",
            "max": 22091032,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32736d503c06497abfae8c0421918255",
            "value": 22091032
          }
        },
        "6feb16f2b2fa4021b1a271e1dd442d04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d5dff8bca14435a88fa1814533acd85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "beb7a6fe34b840899bb79c062681696f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c341e1d3bf3b40d1821ce392eb966c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d327cde5a85a4a51bb8b1b3e9cf06c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d5136c19e7645ca9bc8f51ceffb2be1",
            "max": 819257867,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d5dff8bca14435a88fa1814533acd85",
            "value": 819257867
          }
        },
        "d5ef1cb2cbed4b87b3c5d292ff2b0da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6feb16f2b2fa4021b1a271e1dd442d04",
            "placeholder": "​",
            "style": "IPY_MODEL_2919396dbd4b4c8e821d12bd28665d8a",
            "value": " 781M/781M [00:12&lt;00:00, 65.5MB/s]"
          }
        },
        "e257738711f54d5280c8393d9d3dce1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e639132395d64d70b99d8b72c32f8fbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6459e0bcee449b090fc9807672725bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_660afee173694231a6dce3cd94df6cae",
              "IPY_MODEL_261218485cef48df961519dde5edfcbe"
            ],
            "layout": "IPY_MODEL_c341e1d3bf3b40d1821ce392eb966c68"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
